{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n",
      "The cythonmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext cythonmagic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, reprlib, sys\n",
    "\n",
    "%load_ext cython\n",
    "%load_ext cythonmagic\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_confusion import ConfusionMatrix\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "\n",
    "# n.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65     3  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65     3  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  BETWEENNESS  \\\n",
       "0   4.4    n    y    n    n    y  06/19/09 03:21 PM          180      14861.6   \n",
       "1   4.4    n    y    n    n    y  07/02/09 08:41 AM          180      14861.6   \n",
       "\n",
       "   NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0         93.29     0.03      15661        0.49           0.1  \n",
       "1         93.29     0.03      15661        0.49           0.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", parse_dates=True, infer_datetime_format=True, \n",
    "            sep = None, encoding = \"latin-1\", engine = \"python\")\n",
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'n': \"no\", 'y': \"yes\"}\n",
    "\n",
    "# http://stackoverflow.com/a/17702781\n",
    "data = data.replace(d)\n",
    "\n",
    "neu = data[[\"#AUTHID\",\"STATUS\",\"cNEU\"]]\n",
    "ext = data[[\"#AUTHID\",\"STATUS\",\"cEXT\"]]\n",
    "agr = data[[\"#AUTHID\",\"STATUS\",\"cAGR\"]]\n",
    "con = data[[\"#AUTHID\",\"STATUS\",\"cCON\"]]\n",
    "opn = data[[\"#AUTHID\",\"STATUS\",\"cOPN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6545\n"
     ]
    }
   ],
   "source": [
    "# plit data\n",
    "train_neu, test_neu = sk.cross_validation.train_test_split(neu, train_size = 0.66)\n",
    "train_ext, test_ext = sk.cross_validation.train_test_split(ext, train_size = 0.66)\n",
    "train_agr, test_agr = sk.cross_validation.train_test_split(agr, train_size = 0.66)\n",
    "train_con, test_con = sk.cross_validation.train_test_split(con, train_size = 0.66)\n",
    "train_opn, test_opn = sk.cross_validation.train_test_split(opn, train_size = 0.66)\n",
    "print(len(train_neu))\n",
    "\n",
    "# http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html\n",
    "# http://glowingpython.blogspot.de/2013/07/combining-scikit-learn-and-ntlk.html\n",
    "# http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "# https://stackoverflow.com/questions/10526579/use-scikit-learn-to-classify-into-multiple-categories\n",
    "# https://github.com/anuraagvak/IRE-PersonalityRecognition-Final/blob/master/ire_report.pdf\n",
    "# https://github.com/Charudatt89/Personality_Recognition/blob/master/22-9-PersonalityRecognition/Report/Report.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.592       0.57384342  0.58860196]\n",
      "['yes' 'no' 'yes' ..., 'no' 'yes' 'no']\n",
      "3372\n"
     ]
    }
   ],
   "source": [
    "target_names = ['yes', 'no']\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "\n",
    "clas_fit = classifier.fit(train_neu[\"STATUS\"], train_neu[\"cNEU\"])\n",
    "predicted = classifier.predict(test_neu[\"cNEU\"])\n",
    "\n",
    "print(cross_validation.cross_val_score(classifier, test_neu[\"STATUS\"], predicted))\n",
    "\n",
    "print(predicted)\n",
    "\n",
    "count = 0\n",
    "for item, labels in zip(test_neu[\"STATUS\"], predicted):\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted   no  yes  __all__\n",
      "Actual                      \n",
      "no         389  214      603\n",
      "yes        317  208      525\n",
      "__all__    706  422     1128\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Accuracy: 0.529255319149\n",
      "95% CI: (0.49963934814802252, 0.55871834870459092)\n",
      "No Information Rate: ToDo\n",
      "P-Value [Acc > NIR]: 0.999999999985\n",
      "Kappa: 0.0418290365308\n",
      "Mcnemar's Test P-Value: ToDo\n",
      "\n",
      "\n",
      "Class Statistics:\n",
      "\n",
      "Classes                                       no        yes\n",
      "Population                                  1128       1128\n",
      "P: Condition positive                        603        525\n",
      "N: Condition negative                        525        603\n",
      "Test outcome positive                        706        422\n",
      "Test outcome negative                        422        706\n",
      "TP: True Positive                            389        208\n",
      "TN: True Negative                            208        389\n",
      "FP: False Positive                           317        214\n",
      "FN: False Negative                           214        317\n",
      "TPR: (Sensitivity, hit rate, recall)    0.645108    0.39619\n",
      "TNR=SPC: (Specificity)                   0.39619   0.645108\n",
      "PPV: Pos Pred Value (Precision)         0.550992   0.492891\n",
      "NPV: Neg Pred Value                     0.492891   0.550992\n",
      "FPR: False-out                           0.60381   0.354892\n",
      "FDR: False Discovery Rate               0.449008   0.507109\n",
      "FNR: Miss Rate                          0.354892    0.60381\n",
      "ACC: Accuracy                           0.529255   0.529255\n",
      "F1 score                                0.594347   0.439282\n",
      "MCC: Matthews correlation coefficient  0.0425708  0.0425708\n",
      "Informedness                           0.0412983  0.0412983\n",
      "Markedness                             0.0438825  0.0438825\n",
      "Prevalence                              0.534574   0.465426\n",
      "LR+: Positive likelihood ratio            1.0684    1.11637\n",
      "LR-: Negative likelihood ratio          0.895762   0.935982\n",
      "DOR: Diagnostic odds ratio               1.19272    1.19272\n",
      "FOR: False omission rate                0.507109   0.449008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f931291ceb8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAHcCAYAAAC+pSKAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmIWDJEkamIWDJEka2P8H7IuWkBU6/PwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f930fb679e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = ConfusionMatrix(test_neu[\"cNEU\"], predicted)\n",
    "confusion_matrix.print_stats()\n",
    "\n",
    "confusion_matrix.plot(normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Rules</strong>\n",
    "    * Each single sentence counts (but no weird symbols incl. punc.) and lowercase\n",
    "    * We do use lemmalization, deleting words etc.\n",
    "    \n",
    "##### Meeting with Dustin:    \n",
    "    * Split dataset according to traits, with one column status and another one the yes/no variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tagger = n.ClassifierBasedPOSTagger(train=train_neu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines_of_trained_dataset = train[\"STATUS\"].str.lower()\n",
    "lines_of_trained_dataset.to_csv(\"train.csv\")\n",
    "# print(lines_of_combined_dataset.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "twt = n.TreebankWordTokenizer()\n",
    "\n",
    "# Create lists\n",
    "list_of_rows_CD = [l.split(\"\\n\")[0] for l in lines_of_trained_dataset]\n",
    "\n",
    "list_of_splitted_words_CD, list_of_sentences = [], []\n",
    "\n",
    "for k in list_of_rows_CD:\n",
    "    list_of_sentences.append(n.sent_tokenize(k))\n",
    "    \n",
    "for kCD in list_of_rows_CD:\n",
    "    list_of_splitted_words_CD.append(n.word_tokenize(kCD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['will', 'try', 'my', 'hands', 'at', 'sushi', 'makin', \"'\", ':', 'p'], ['whilst', 'i', 'may', 'not', 'understand', 'the', 'history', 'behind', 'thanksgiving', ',', 'i', 'can', 'get', 'behind', 'any', 'holiday', 'that', 'consists', 'of', 'a', 'huge', 'amount', 'of', 'food', ',', 'alcohol', 'and', 'friends', '.', 'happy', 'thanksgiving', 'to', 'my', 'american', 'friends', '.', 'and', 'to', 'anyone', 'else', ',', 'today', 'is', 'just', 'an', 'excuse', 'for', 'us', 'to', 'take', 'part', 'in', '``', 'cultural', 'diversity', \"''\", 'and', 'get', 'drunk', 'and', 'stuffed', 'with', 'lots', 'of', 'food', '.']] \n",
      "\n",
      " [\"will try my hands at sushi makin' :p\", 'whilst i may not understand the history behind thanksgiving, i can get behind any holiday that consists of a huge amount of food, alcohol and friends. happy thanksgiving to my american friends. and to anyone else, today is just an excuse for us to take part in \"cultural diversity\" and get drunk and stuffed with lots of food.'] \n",
      "\n",
      " [[\"will try my hands at sushi makin' :p\"], ['whilst i may not understand the history behind thanksgiving, i can get behind any holiday that consists of a huge amount of food, alcohol and friends.', 'happy thanksgiving to my american friends.', 'and to anyone else, today is just an excuse for us to take part in \"cultural diversity\" and get drunk and stuffed with lots of food.']]\n"
     ]
    }
   ],
   "source": [
    "print(list_of_splitted_words_CD[:2], \"\\n\\n\", list_of_rows_CD[:2], \"\\n\\n\", list_of_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 116764 tokens in 6545 sentences\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/2058985/python-count-sub-lists-in-nested-list\n",
    "print(\"there are\", sum(len(x) for x in list_of_splitted_words_CD), \"tokens in\", len(list_of_sentences), \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(list_of_rows_CD[:2], \"\\n\")\n",
    "#print(list_of_sentences[:2],\"\\n\") \n",
    "#print(list_of_splitted_words_CD[:2], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different tokenizers (will decide later on which one)\n",
    "\n",
    "Source: http://text-processing.com/demo/tokenize/\n",
    "\n",
    "\n",
    "<!-- <img src=\"dif_tokenizers.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer \n",
    "\n",
    "##### https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
    "\n",
    "> Stemming is a technique to remove affixes from a word, ending up with the stem. For\n",
    "example, the stem of cooking is cook, and a good stemming algorithm knows that the ing\n",
    "suffix can be removed.\n",
    "\n",
    "\n",
    "### Lemmatization \n",
    "is very similar to stemming, but is more akin to synonym replacement. A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always left with a valid word that means the same thing. However, the word you end up with can be completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \n",
      " [[\"will try my ...shi makin' :p\"], ['whilst i may... and friends.', 'happy thanks...ican friends.', 'and to anyon...lots of food.'], ['wants to kno...stume off him'], ['is stoked th... - in denver!', 'ah...memorie... cambridge...'], ['just watched...o have talent'], ['ewww 33\" of ...e since 1969!'], ...] \n",
      "\n",
      "Lemmatizer \n",
      " [['will', 'try', 'my', 'hand', 'at', 'sushi', ...], ['whilst', 'i', 'may', 'not', 'understand', 'the', ...], ['want', 'to', 'know', 'if', 'anyone', 'ha', ...], ['is', 'stoked', 'that', 'she', 'is', 'going', ...], ['just', 'watched', 'the', '*propname*', 'press', 'conference', ...], ['ewww', '33', \"''\", 'of', 'snow', '...', ...], ...] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Huge nested lists\n",
    "# http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet\n",
    "\n",
    "nest_list_lm = [[] for _ in range(len(list_of_splitted_words_CD))]\n",
    "\n",
    "lm = n.stem.WordNetLemmatizer()\n",
    "\n",
    "for sentence in list_of_splitted_words_CD: \n",
    "    for word in sentence:\n",
    "        nest_list_lm[list_of_splitted_words_CD.index(sentence)].append(lm.lemmatize(word))\n",
    "\n",
    "print(\"Original \\n\", reprlib.repr(list_of_sentences), \"\\n\")\n",
    "print(\"Lemmatizer \\n\", reprlib.repr(nest_list_lm), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdfword'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# book 38\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):        \n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "    \n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "        \n",
    "    def delete_stupid_chars(self, word):\n",
    "        \"\"\"\n",
    "        http://stackoverflow.com/a/3874768\n",
    "        used above\n",
    "        replaced_word = self.replace(word)\n",
    "        \"\"\"\n",
    "        rem = \"!?#.,();:'[].,//``...~<>$%^&*-_-=+\"\n",
    "        return word.translate(str.maketrans(dict.fromkeys(rem)))\n",
    "\n",
    "replacer = RepeatReplacer()\n",
    "replacer.delete_stupid_chars(\"!?sdf,word!??)()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def delete_repChars():\n",
    "    nest_list_lm_repchars = [[] for _ in range(len(nest_list_lm))]\n",
    "    for sentence in nest_list_lm:\n",
    "        for word in sentence:\n",
    "            nest_list_lm_repchars[nest_list_lm.index(sentence)].append(replacer.delete_stupid_chars(word))\n",
    "    return nest_list_lm_repchars\n",
    "\n",
    "def delete_empty_strings():\n",
    "    nest = [[] for _ in range(len(nest_list_lm_repchars))]\n",
    "    for sentence in nest_list_lm_repchars: \n",
    "        for word in sentence:\n",
    "            if word != '':\n",
    "                nest[nest_list_lm_repchars.index(sentence)].append(word)\n",
    "    return nest\n",
    "\n",
    "nest_list_lm_repchars = delete_repChars()\n",
    "nest = delete_empty_strings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['will', 'try', 'my', 'hand', 'at', 'sushi', ...], ['whilst', 'i', 'may', 'not', 'understand', 'the', ...], ['want', 'to', 'know', 'if', 'anyone', 'ha', ...], ['is', 'stoked', 'that', 'she', 'is', 'going', ...], ['just', 'watched', 'the', 'propname', 'press', 'conference', ...], ['ewww', '33', '', 'of', 'snow', '', ...], ...] \n",
      "\n",
      " ['will try my ...sushi makin p', 'whilst i may...h lot of food', 'want to know...stume off him', 'is stoked th... in cambridge', 'just watched...o have talent', 'ewww 33 of s...me since 1969', ...] \n",
      "\n",
      " [['will', 'try', 'my', 'hand', 'at', 'sushi', ...], ['whilst', 'i', 'may', 'not', 'understand', 'the', ...], ['want', 'to', 'know', 'if', 'anyone', 'ha', ...], ['is', 'stoked', 'that', 'she', 'is', 'going', ...], ['just', 'watched', 'the', 'propname', 'press', 'conference', ...], ['ewww', '33', 'of', 'snow', 'apparently', 'the', ...], ...]\n"
     ]
    }
   ],
   "source": [
    "outlst = [' '.join([str(c) for c in hm]) for hm in nest]\n",
    "\n",
    "print(reprlib.repr(nest_list_lm_repchars), \"\\n\\n\", reprlib.repr(outlst), \"\\n\\n\", reprlib.repr(nest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devil: 7.2216600976\n"
     ]
    }
   ],
   "source": [
    "#http://aylien.com/web-summit-2015-tweets-part1\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_features = 10000)\n",
    "vz = vectorizer.fit_transform(outlst)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "print(\"devil: \" + str(tfidf[\"devil\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.plaintext.PlaintextCorpusReader'>\n",
      "/home/jm/Documents/python-notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: File `'train_tagger.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "os.chdir(\"/home/jm/Documents/python-notebook/\")\n",
    "newcorpus = PlaintextCorpusReader(\"./\", '.*\\.txt')\n",
    "print(type(newcorpus))\n",
    "\n",
    "#os.chdir(\"/home/jm/Documents/nltk-trainer\")\n",
    "print(os.getcwd())\n",
    "%run train_tagger.py newcorpus --fraction 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8860677839274426"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank, brown\n",
    "from nltk.tag import *\n",
    "from nltk.tag.util import *\n",
    "\n",
    "# backoff = DefaultTagger('NN')\n",
    "# tagger = BackoffTagger([UnigramTagger, BigramTagger,TrigramTagger])\n",
    "# print(brown.sents(categories='news')[0])\n",
    "# print(brown.tagged_sents(categories='news')[0])\n",
    "\n",
    "# tagger1 = DefaultTagger('NN')\n",
    "# t1 = tagger1.tag_sents(nest)\n",
    "# print(t1[0])\n",
    "\n",
    "# bitagger = BigramTagger(t1)\n",
    "# tritagger = TrigramTagger(train_sents)\n",
    "\n",
    "train_sents =  treebank.tagged_sents()[3000:]\n",
    "test_sents =  treebank.tagged_sents()[:3000]\n",
    "\n",
    "#print(test_sents[0])\n",
    "tagger = n.ClassifierBasedPOSTagger(train=train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tagged words from all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will try my hands at sushi makin' :p\n",
      "['will', 'try', 'my', 'hands', 'at', 'sushi', 'makin', \"'\", ':', 'p']\n",
      "[\"will try my hands at sushi makin' :p\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(reprlib.repr(nest_list_without_stopwords_lm))\n",
    "\n",
    "print(list_of_rows_CD[0])\n",
    "print(list_of_splitted_words_CD[0])\n",
    "print(list_of_sentences[0], \"\\n\")\n",
    "\n",
    "#tag_words = n.pos_tag(list_of_splitted_words_CD)\n",
    "#print(tag_words, \"\\n\")\n",
    "# print(reprlib.repr(tagged_words()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nest_list_without_stopwords_lm_repchars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-73a1677bafe0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnest_list_tagged_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest_list_without_stopwords_lm_repchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#def tagged_words():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#    for sentence in nest_list_without_stopwords_lm_repchars:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#        for words in sentence:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nest_list_without_stopwords_lm_repchars' is not defined"
     ]
    }
   ],
   "source": [
    "nest_list_tagged_words = [[] for _ in range(len(nest_list_without_stopwords_lm_repchars))]\n",
    "\n",
    "#def tagged_words():\n",
    "#    for sentence in nest_list_without_stopwords_lm_repchars:\n",
    "#        for words in sentence:\n",
    "#            nest_list_tagged_words[nest_list_without_stopwords_lm_repchars.index(sentence)].append(n.pos_tag(words))\n",
    "#    return nest_list_tagged_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram features\n",
    "\n",
    "Use -a for code analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-60c1a386ae7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-60c1a386ae7b>\u001b[0m in \u001b[0;36mword_fea\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "def word_fea(words):\n",
    "    return dict((word, True))\n",
    "word_fea(outlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bigram collocation\n",
    "# https://github.com/neotenic/cancer/blob/master/nltk.ipynb\n",
    "def bigram_features(words, score_fn=BAM.chi_sq): \n",
    "    bg_finder = BigramCollocationFinder.from_words(words) \n",
    "    bigrams = bg_finder.nbest(score_fn, 100000) \n",
    "    return dict((bg, True) for bg in chain(words, bigrams)) \n",
    "\n",
    "#bigram_features(outlst, score_fn=BAM.chi_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e7dd2442d0e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%.3f\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;31m# the label and featurename.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "cl = n.NaiveBayesClassifier.train(train)\n",
    "print(n.classify.accuracy(cl, test),\"%.3f\")\n",
    "cl.show_most_informative_features(40)\n",
    "cl.prob_classify(featurize(name)) #"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
