{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n",
      "The cythonmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext cythonmagic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, reprlib, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext cython\n",
    "%load_ext cythonmagic\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas_confusion import ConfusionMatrix\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem\n",
    "from nltk.corpus import stopwords, wordnet, treebank, brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "from nltk.tag import *\n",
    "\n",
    "from nltk.tag.util import *\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# n.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65     3  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65     3  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  BETWEENNESS  \\\n",
       "0   4.4    n    y    n    n    y  06/19/09 03:21 PM          180      14861.6   \n",
       "1   4.4    n    y    n    n    y  07/02/09 08:41 AM          180      14861.6   \n",
       "\n",
       "   NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0         93.29     0.03      15661        0.49           0.1  \n",
       "1         93.29     0.03      15661        0.49           0.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", parse_dates=True, infer_datetime_format=True, \n",
    "            sep = None, encoding = \"latin-1\", engine = \"python\")\n",
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'n': \"no\", 'y': \"yes\"}\n",
    "\n",
    "# http://stackoverflow.com/a/17702781\n",
    "data = data.replace(d)\n",
    "\n",
    "neu = data[[\"#AUTHID\",\"STATUS\",\"cNEU\"]]\n",
    "ext = data[[\"#AUTHID\",\"STATUS\",\"cEXT\"]]\n",
    "agr = data[[\"#AUTHID\",\"STATUS\",\"cAGR\"]]\n",
    "con = data[[\"#AUTHID\",\"STATUS\",\"cCON\"]]\n",
    "opn = data[[\"#AUTHID\",\"STATUS\",\"cOPN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6545\n"
     ]
    }
   ],
   "source": [
    "# plit data\n",
    "train_neu, test_neu = sk.cross_validation.train_test_split(neu, train_size = 0.66)\n",
    "train_ext, test_ext = sk.cross_validation.train_test_split(ext, train_size = 0.66)\n",
    "train_agr, test_agr = sk.cross_validation.train_test_split(agr, train_size = 0.66)\n",
    "train_con, test_con = sk.cross_validation.train_test_split(con, train_size = 0.66)\n",
    "train_opn, test_opn = sk.cross_validation.train_test_split(opn, train_size = 0.66)\n",
    "print(len(train_neu))\n",
    "\n",
    "# http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html\n",
    "# http://glowingpython.blogspot.de/2013/07/combining-scikit-learn-and-ntlk.html\n",
    "# http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "# https://stackoverflow.com/questions/10526579/use-scikit-learn-to-classify-into-multiple-categories\n",
    "# https://github.com/anuraagvak/IRE-PersonalityRecognition-Final/blob/master/ire_report.pdf\n",
    "# https://github.com/Charudatt89/Personality_Recognition/blob/master/22-9-PersonalityRecognition/Report/Report.pdf\n",
    "# http://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56 (+/- 0.01)\n",
      "Confusion Matrix:\n",
      "\n",
      "Predicted   no  yes  __all__\n",
      "Actual                      \n",
      "no         360  230      590\n",
      "yes        317  202      519\n",
      "__all__    677  432     1109\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Accuracy: 0.506762849414\n",
      "95% CI: (0.47690253676825528, 0.53658719736965199)\n",
      "No Information Rate: ToDo\n",
      "P-Value [Acc > NIR]: 0.999999999999\n",
      "Kappa: -0.000626811361121\n",
      "Mcnemar's Test P-Value: ToDo\n",
      "\n",
      "\n",
      "Class Statistics:\n",
      "\n",
      "Classes                                         no          yes\n",
      "Population                                    1109         1109\n",
      "P: Condition positive                          590          519\n",
      "N: Condition negative                          519          590\n",
      "Test outcome positive                          677          432\n",
      "Test outcome negative                          432          677\n",
      "TP: True Positive                              360          202\n",
      "TN: True Negative                              202          360\n",
      "FP: False Positive                             317          230\n",
      "FN: False Negative                             230          317\n",
      "TPR: (Sensitivity, hit rate, recall)      0.610169      0.38921\n",
      "TNR=SPC: (Specificity)                     0.38921     0.610169\n",
      "PPV: Pos Pred Value (Precision)           0.531758     0.467593\n",
      "NPV: Neg Pred Value                       0.467593     0.531758\n",
      "FPR: False-out                             0.61079     0.389831\n",
      "FDR: False Discovery Rate                 0.468242     0.532407\n",
      "FNR: Miss Rate                            0.389831      0.61079\n",
      "ACC: Accuracy                             0.506763     0.506763\n",
      "F1 score                                  0.568272     0.424816\n",
      "MCC: Matthews correlation coefficient -0.000634903 -0.000634903\n",
      "Informedness                          -0.000620489 -0.000620489\n",
      "Markedness                            -0.000649653 -0.000649653\n",
      "Prevalence                                0.532011     0.467989\n",
      "LR+: Positive likelihood ratio            0.998984     0.998408\n",
      "LR-: Negative likelihood ratio             1.00159      1.00102\n",
      "DOR: Diagnostic odds ratio                0.997394     0.997394\n",
      "FOR: False omission rate                  0.532407     0.468242\n"
     ]
    }
   ],
   "source": [
    "target_names = ['yes', 'no']\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    #('nb', MultinomialNB())])\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB()))])\n",
    "\n",
    "\n",
    "clas_fit = classifier.fit(train_neu[\"STATUS\"], train_neu[\"cNEU\"])\n",
    "predicted = classifier.predict(test_neu[\"cNEU\"])\n",
    "\n",
    "#scores = cross_validation.cross_val_score(classifier, neu[\"STATUS\"], neu[\"cNEU\"], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "count = 0\n",
    "for item, labels in zip(test_neu[\"STATUS\"], predicted):\n",
    "    count += 1\n",
    "#print(count)\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(test_neu[\"cNEU\"], predicted)\n",
    "confusion_matrix.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe21df442e8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAHhCAYAAAAPqpaJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe221a3b160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix.plot(normalized=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram/Trigram training/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-d6301a3fa129>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-d6301a3fa129>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    tritagger.\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tritagger = TrigramTagger(train_sents)\n",
    "tritagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Rules</strong>\n",
    "    * Each single sentence counts (but no weird symbols incl. punc.) and lowercase\n",
    "    * We do use lemmalization, deleting words etc.\n",
    "    \n",
    "##### Meeting with Dustin:    \n",
    "    * Split dataset according to traits, with one column status and another one the yes/no variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tagger = n.ClassifierBasedPOSTagger(train=train_neu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines_of_trained_dataset = train_neu[\"STATUS\"] # .str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "twt = n.TreebankWordTokenizer()\n",
    "\n",
    "# Create lists\n",
    "list_of_rows_CD = [l.split(\"\\n\")[0] for l in lines_of_trained_dataset]\n",
    "\n",
    "list_of_splitted_words_CD, list_of_sentences = [], []\n",
    "\n",
    "for k in list_of_rows_CD:\n",
    "    list_of_sentences.append(n.sent_tokenize(k))\n",
    "    \n",
    "for kCD in list_of_rows_CD:\n",
    "    list_of_splitted_words_CD.append(n.word_tokenize(kCD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['People', 'demand', 'freedom', 'of', 'speech', 'to', 'make', 'up', 'for', 'the', 'freedom', 'of', 'thought', 'which', 'they', 'avoid'], ['is', 'excited', 'for', 'being', 'able', 'to', 'spot', 'the', 'Summer', 'Triangle', '.', 'Not', 'to', 'be', 'confused', 'with', 'the', 'Bermuda', 'Triangle', '.']] \n",
      "\n",
      " ['People demand freedom of speech to make up  for the freedom of thought which they avoid', 'is excited for being able to spot the Summer Triangle. Not to be confused with the Bermuda Triangle.'] \n",
      "\n",
      " [['People demand freedom of speech to make up  for the freedom of thought which they avoid'], ['is excited for being able to spot the Summer Triangle.', 'Not to be confused with the Bermuda Triangle.']]\n"
     ]
    }
   ],
   "source": [
    "print(list_of_splitted_words_CD[:2], \"\\n\\n\", list_of_rows_CD[:2], \"\\n\\n\", list_of_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 116979 tokens in 6545 sentences\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/2058985/python-count-sub-lists-in-nested-list\n",
    "print(\"there are\", sum(len(x) for x in list_of_splitted_words_CD), \"tokens in\", len(list_of_sentences), \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(list_of_rows_CD[:2], \"\\n\")\n",
    "#print(list_of_sentences[:2],\"\\n\") \n",
    "#print(list_of_splitted_words_CD[:2], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different tokenizers (will decide later on which one)\n",
    "\n",
    "Source: http://text-processing.com/demo/tokenize/\n",
    "\n",
    "\n",
    "<!-- <img src=\"dif_tokenizers.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer \n",
    "\n",
    "##### https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
    "\n",
    "> Stemming is a technique to remove affixes from a word, ending up with the stem. For\n",
    "example, the stem of cooking is cook, and a good stemming algorithm knows that the ing\n",
    "suffix can be removed.\n",
    "\n",
    "\n",
    "### Lemmatization \n",
    "is very similar to stemming, but is more akin to synonym replacement. A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always left with a valid word that means the same thing. However, the word you end up with can be completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \n",
      " [['People deman...ch they avoid'], ['is excited f...mer Triangle.', 'Not to be co...uda Triangle.'], ['wonders how ...figure it out'], ['Just ate a n...ne reduction.', 'Wish my feet...eally cook !!', 'Tomorrow we ...illet and me.'], ['learns more ...s everything.'], ['\"*PROPNAME*\"...middle names.', 'Everyone calm down now...'], ...] \n",
      "\n",
      "Lemmatizer \n",
      " [['People', 'demand', 'freedom', 'of', 'speech', 'to', ...], ['is', 'excited', 'for', 'being', 'able', 'to', ...], ['wonder', 'how', 'I', 'keep', 'getting', 'convinced', ...], ['Just', 'ate', 'a', 'nice', 'happy', 'cow', ...], ['learns', 'more', 'and', 'more', 'every', 'day', ...], ['``', '*PROPNAME*', \"''\", 'and', '``', '*PROPNAME*', ...], ...] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Huge nested lists\n",
    "# http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet\n",
    "\n",
    "nest_list_lm = [[] for _ in range(len(list_of_splitted_words_CD))]\n",
    "\n",
    "lm = n.stem.WordNetLemmatizer()\n",
    "\n",
    "for sentence in list_of_splitted_words_CD: \n",
    "    for word in sentence:\n",
    "        nest_list_lm[list_of_splitted_words_CD.index(sentence)].append(lm.lemmatize(word))\n",
    "\n",
    "print(\"Original \\n\", reprlib.repr(list_of_sentences), \"\\n\")\n",
    "print(\"Lemmatizer \\n\", reprlib.repr(nest_list_lm), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdfword'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# book 38\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):        \n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "    \n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "        \n",
    "    def delete_stupid_chars(self, word):\n",
    "        \"\"\"\n",
    "        http://stackoverflow.com/a/3874768\n",
    "        used above\n",
    "        replaced_word = self.replace(word)\n",
    "        \"\"\"\n",
    "        rem = \"!?#.,();:'[].,//``...~<>$%^&*-_-=+\"\n",
    "        return word.translate(str.maketrans(dict.fromkeys(rem)))\n",
    "\n",
    "replacer = RepeatReplacer()\n",
    "replacer.delete_stupid_chars(\"!?sdf,word!??)()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def delete_repChars():\n",
    "    nest_list_lm_repchars = [[] for _ in range(len(nest_list_lm))]\n",
    "    for sentence in nest_list_lm:\n",
    "        for word in sentence:\n",
    "            nest_list_lm_repchars[nest_list_lm.index(sentence)].append(replacer.delete_stupid_chars(word))\n",
    "    return nest_list_lm_repchars\n",
    "\n",
    "def delete_empty_strings():\n",
    "    nest = [[] for _ in range(len(nest_list_lm_repchars))]\n",
    "    for sentence in nest_list_lm_repchars: \n",
    "        for word in sentence:\n",
    "            if word != '':\n",
    "                nest[nest_list_lm_repchars.index(sentence)].append(word)\n",
    "    return nest\n",
    "\n",
    "nest_list_lm_repchars = delete_repChars()\n",
    "nest = delete_empty_strings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f2a5b8e4b2a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_splitted_words_CD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0munitagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#bitagger = n.BigramTagger(train_neu[\"STATUS\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[0;32m    105\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \"\"\"\n\u001b[0;32m     88\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;31m# Standard word tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1224\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m         \"\"\"\n\u001b[1;32m-> 1226\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \"\"\"\n\u001b[1;32m-> 1274\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \"\"\"\n\u001b[0;32m   1303\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1305\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m     \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "word_tokens = n.word_tokenize(list_of_splitted_words_CD)\n",
    "print(word_tokens)\n",
    "\n",
    "unitagger = n.UnigramTagger()\n",
    "#bitagger = n.BigramTagger(train_neu[\"STATUS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['People', 'demand', 'freedom', 'of', 'speech', 'to', ...], ['is', 'excited', 'for', 'being', 'able', 'to', ...], ['wonder', 'how', 'I', 'keep', 'getting', 'convinced', ...], ['Just', 'ate', 'a', 'nice', 'happy', 'cow', ...], ['learns', 'more', 'and', 'more', 'every', 'day', ...], ['', 'PROPNAME', '', 'and', '', 'PROPNAME', ...], ...] \n",
      "\n",
      " ['People deman...ch they avoid', 'is excited f...muda Triangle', 'wonder how I...figure it out', 'Just ate a n...killet and me', 'learns more ...is everything', 'PROPNAME and...calm down now', ...] \n",
      "\n",
      " [['People', 'demand', 'freedom', 'of', 'speech', 'to', ...], ['is', 'excited', 'for', 'being', 'able', 'to', ...], ['wonder', 'how', 'I', 'keep', 'getting', 'convinced', ...], ['Just', 'ate', 'a', 'nice', 'happy', 'cow', ...], ['learns', 'more', 'and', 'more', 'every', 'day', ...], ['PROPNAME', 'and', 'PROPNAME', 'are', 'my', 'two', ...], ...]\n"
     ]
    }
   ],
   "source": [
    "outlst = [' '.join([str(c) for c in hm]) for hm in nest]\n",
    "\n",
    "print(reprlib.repr(nest_list_lm_repchars), \"\\n\\n\", reprlib.repr(outlst), \"\\n\\n\", reprlib.repr(nest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devil: 7.58938487772\n"
     ]
    }
   ],
   "source": [
    "#http://aylien.com/web-summit-2015-tweets-part1\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_features = 10000)\n",
    "vz = vectorizer.fit_transform(outlst)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "print(\"devil: \" + str(tfidf[\"devil\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8860677839274426"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backoff = DefaultTagger('NN')\n",
    "# tagger = BackoffTagger([UnigramTagger, BigramTagger,TrigramTagger])\n",
    "# print(brown.sents(categories='news')[0])\n",
    "# print(brown.tagged_sents(categories='news')[0])\n",
    "\n",
    "# tagger1 = DefaultTagger('NN')\n",
    "# t1 = tagger1.tag_sents(nest)\n",
    "# print(t1[0])\n",
    "\n",
    "# bitagger = BigramTagger(t1)\n",
    "# tritagger = TrigramTagger(train_sents)\n",
    "\n",
    "train_sents =  treebank.tagged_sents()[3000:]\n",
    "test_sents =  treebank.tagged_sents()[:3000]\n",
    "\n",
    "#print(test_sents[0])\n",
    "tagger = n.ClassifierBasedPOSTagger(train=train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tagged words from all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People demand freedom of speech to make up  for the freedom of thought which they avoid\n",
      "['People', 'demand', 'freedom', 'of', 'speech', 'to', 'make', 'up', 'for', 'the', 'freedom', 'of', 'thought', 'which', 'they', 'avoid']\n",
      "['People demand freedom of speech to make up  for the freedom of thought which they avoid'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(reprlib.repr(nest_list_without_stopwords_lm))\n",
    "\n",
    "print(list_of_rows_CD[0])\n",
    "print(list_of_splitted_words_CD[0])\n",
    "print(list_of_sentences[0], \"\\n\")\n",
    "\n",
    "#tag_words = n.pos_tag(list_of_splitted_words_CD)\n",
    "#print(tag_words, \"\\n\")\n",
    "# print(reprlib.repr(tagged_words()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nest_list_without_stopwords_lm_repchars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-73a1677bafe0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnest_list_tagged_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest_list_without_stopwords_lm_repchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#def tagged_words():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#    for sentence in nest_list_without_stopwords_lm_repchars:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#        for words in sentence:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nest_list_without_stopwords_lm_repchars' is not defined"
     ]
    }
   ],
   "source": [
    "nest_list_tagged_words = [[] for _ in range(len(nest_list_without_stopwords_lm_repchars))]\n",
    "\n",
    "#def tagged_words():\n",
    "#    for sentence in nest_list_without_stopwords_lm_repchars:\n",
    "#        for words in sentence:\n",
    "#            nest_list_tagged_words[nest_list_without_stopwords_lm_repchars.index(sentence)].append(n.pos_tag(words))\n",
    "#    return nest_list_tagged_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram features\n",
    "\n",
    "Use -a for code analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-60c1a386ae7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-60c1a386ae7b>\u001b[0m in \u001b[0;36mword_fea\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "def word_fea(words):\n",
    "    return dict((word, True))\n",
    "word_fea(outlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bigram collocation\n",
    "# https://github.com/neotenic/cancer/blob/master/nltk.ipynb\n",
    "def bigram_features(words, score_fn=BAM.chi_sq): \n",
    "    bg_finder = BigramCollocationFinder.from_words(words) \n",
    "    bigrams = bg_finder.nbest(score_fn, 100000) \n",
    "    return dict((bg, True) for bg in chain(words, bigrams)) \n",
    "\n",
    "#bigram_features(outlst, score_fn=BAM.chi_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e7dd2442d0e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%.3f\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "cl = n.NaiveBayesClassifier.train(train)\n",
    "print(n.classify.accuracy(cl, test),\"%.3f\")\n",
    "cl.show_most_informative_features(40)\n",
    "cl.prob_classify(featurize(name)) #"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
