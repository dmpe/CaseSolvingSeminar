{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/12761510/python-how-can-i-calculate-the-average-word-length-in-a-sentence-using-the-spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/IPython/extensions/cythonmagic.py:21: UserWarning: The Cython magic has been moved to the Cython package\n",
      "  warnings.warn(\"\"\"The Cython magic has been moved to the Cython package\"\"\")\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/qda.py:4: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, reprlib, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import *\n",
    "import random as ran\n",
    "ran.seed(5125)\n",
    "\n",
    "%load_ext cython\n",
    "%load_ext cythonmagic\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas_confusion import ConfusionMatrix\n",
    "from scipy.cluster.vq import whiten\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem\n",
    "from nltk.corpus import *\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from nltk.probability import *\n",
    "from nltk.classify.scikitlearn import *\n",
    "from nltk.tag.sequential import *\n",
    "from nltk.tag import *\n",
    "from nltk.tag.util import *\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID                       STATUS  sEXT  sNEU  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4  likes the sound of thunder.  2.65     3   \n",
       "\n",
       "   sAGR  sCON  sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  \\\n",
       "0  3.15  3.25   4.4    n    y    n    n    y  06/19/09 03:21 PM          180   \n",
       "\n",
       "   BETWEENNESS  NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0      14861.6         93.29     0.03      15661        0.49           0.1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", parse_dates=True, infer_datetime_format=True, \n",
    "            sep = None, encoding = \"latin-1\", engine = \"python\")\n",
    "data.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>...</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "      <th>Lenght_of_Rows_withoutSpaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID                       STATUS  sEXT  sNEU  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4  likes the sound of thunder.  2.65     3   \n",
       "\n",
       "   sAGR  sCON  sOPN cEXT cNEU cAGR              ...              cOPN  \\\n",
       "0  3.15  3.25   4.4    n    y    n              ...                 y   \n",
       "\n",
       "                DATE NETWORKSIZE  BETWEENNESS  NBETWEENNESS  DENSITY  \\\n",
       "0  06/19/09 03:21 PM         180      14861.6         93.29     0.03   \n",
       "\n",
       "   BROKERAGE  NBROKERAGE  TRANSITIVITY  Lenght_of_Rows_withoutSpaces  \n",
       "0      15661        0.49           0.1                           NaN  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"likes the sound of thunder.\"\n",
    "print(sum(c != ' ' for c in word))\n",
    "#data[\"Lenght_of_Rows_withSpaces\"] = None\n",
    "data[\"Lenght_of_Rows_withoutSpaces\"] = np.nan\n",
    "\n",
    "data.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average word length in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df1['e'] = Series(np.random.randn(sLength), index=df1.index) .loc[:,'f']\n",
    "for index, sentence in data.iterrows():\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!?-', sentence[\"STATUS\"]))\n",
    "    words = [word for word in filtered.split() if word]\n",
    "    try:\n",
    "        avg = sum(map(len, words))/len(words)\n",
    "    except ZeroDivisionError:\n",
    "        avg = 0\n",
    "    #print(\"%0.1f\" % avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, sentence in data.iterrows():\n",
    "    #var1 = data[[\"STATUS\"]][sentence.index]\n",
    "    data[\"Lenght_of_Rows_withoutSpaces\"][sentence.index] = sum(c != ' ' for c in sentence[\"STATUS\"])\n",
    "    data[\"Lenght_of_Rows_withSpaces\"][index] = len(sentence[\"STATUS\"])\n",
    "\n",
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'n': \"no\", 'y': \"yes\"}\n",
    "\n",
    "# http://stackoverflow.com/a/17702781\n",
    "data = data.replace(d)\n",
    "\n",
    "neu = data[[\"#AUTHID\",\"STATUS\",\"cNEU\"]]\n",
    "ext = data[[\"#AUTHID\",\"STATUS\",\"cEXT\"]]\n",
    "agr = data[[\"#AUTHID\",\"STATUS\",\"cAGR\"]]\n",
    "con = data[[\"#AUTHID\",\"STATUS\",\"cCON\"]]\n",
    "opn = data[[\"#AUTHID\",\"STATUS\",\"cOPN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_neu, test_neu = sk.cross_validation.train_test_split(neu, train_size = 0.66)\n",
    "train_ext, test_ext = sk.cross_validation.train_test_split(ext, train_size = 0.66)\n",
    "train_agr, test_agr = sk.cross_validation.train_test_split(agr, train_size = 0.66)\n",
    "train_con, test_con = sk.cross_validation.train_test_split(con, train_size = 0.66)\n",
    "train_opn, test_opn = sk.cross_validation.train_test_split(opn, train_size = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-177-83855c43df60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#data[[\"STATUS\"]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for sentence in data: \n",
    "    print(sum(c != ' ' for c in sentence[[2]]))\n",
    "#data[[\"STATUS\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SyntacticFeatures(chapters):\n",
    "    \"\"\"\n",
    "    Extract feature vector for part of speech frequencies\n",
    "    \"\"\"\n",
    "    def token_to_pos(ch):\n",
    "        tokens = n.word_tokenize(ch)\n",
    "        return [p[1] for p in n.pos_tag(tokens)]\n",
    "\n",
    "    chapters_pos = [token_to_pos(ch) for ch in chapters]\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[ch.count(pos) for pos in pos_list]\n",
    "                           for ch in chapters_pos]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens in the chapter\n",
    "    fvs_syntax /= np.c_[np.array([len(ch) for ch in chapters_pos])]\n",
    "\n",
    "    return fvs_syntax\n",
    "\n",
    "feature_sets.append(SyntacticFeatures(neu[[\"STATUS\"]]))\n",
    "feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LexicalFeatures(chapters):\n",
    "\n",
    "    sentence_tokenizer = n.data.load('tokenizers/punkt/english.pickle')\n",
    "    word_tokenizer = n.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for row in chapters.iterrows():\n",
    "        # print(row)\n",
    "        \n",
    "        num_chapters = len(row)\n",
    "        tokens = n.word_tokenize(row.lower())\n",
    "        words = word_tokenizer.tokenize(row.lower())\n",
    "        sentences = sentence_tokenizer.tokenize(row)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                           for s in sentences])\n",
    "        print(\"druhy pokus:\", num_chapters, tokens, words, sentences, vocab, words_per_sentence)\n",
    "\n",
    "        # create an array of zeros https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.zeros.html\n",
    "        #fvs_lexical = np.zeros((len(chapters), 3), np.float64)\n",
    "        #fvs_punct = np.zeros((len(chapters), 3), np.float64)\n",
    "\n",
    "        #print(\"testing:\", num_chapters, chapters, fvs_lexical, fvs_punct)\n",
    "\n",
    "#         for e, ch_text in enumerate(num_chapters):\n",
    "            # note: the nltk.word_tokenize includes punctuation\n",
    "            \n",
    "\n",
    "#             # average number of words per sentence\n",
    "#             fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "#             # sentence length variation\n",
    "#             fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "#             # Lexical diversity\n",
    "#             fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    "\n",
    "#             # Commas per sentence\n",
    "#             fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "#             # Semicolons per sentence\n",
    "#             fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "#             # Colons per sentence\n",
    "#             fvs_punct[e, 2] = tokens.count(':') / float(len(sentences))\n",
    "\n",
    "\n",
    "        # apply whitening to decorrelate the features\n",
    "#         fvs_lexical = whiten(fvs_lexical)\n",
    "#         fvs_punct = whiten(fvs_punct)\n",
    "\n",
    "#     return fvs_lexical, fvs_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LexicalFeatures(neu[[\"STATUS\"]])\n",
    "word_tokenizer = n.tokenize.RegexpTokenizer(r'\\w+')\n",
    "words_per_sentence_AVG = word_tokenizer.tokenize(neu[\"STATUS\"].str.lower())\n",
    "feature_sets = None\n",
    "feature_sets = list(LexicalFeatures(neu[[\"STATUS\"]]))\n",
    "feature_sets"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
