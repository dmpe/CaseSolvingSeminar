{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n",
      "The cythonmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext cythonmagic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, reprlib, sys\n",
    "\n",
    "%load_ext cython\n",
    "%load_ext cythonmagic\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem, nltk.book\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "\n",
    "# n.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/15/09 01:15 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/22/09 04:48 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/20/09 02:31 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65     3  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65     3  3.15  3.25   \n",
       "2  is sore and wants the knot of muscles at the b...  2.65     3  3.15  3.25   \n",
       "3         likes how the day sounds in this new song.  2.65     3  3.15  3.25   \n",
       "4                                        is home. <3  2.65     3  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  BETWEENNESS  \\\n",
       "0   4.4    n    y    n    n    y  06/19/09 03:21 PM          180      14861.6   \n",
       "1   4.4    n    y    n    n    y  07/02/09 08:41 AM          180      14861.6   \n",
       "2   4.4    n    y    n    n    y  06/15/09 01:15 PM          180      14861.6   \n",
       "3   4.4    n    y    n    n    y  06/22/09 04:48 AM          180      14861.6   \n",
       "4   4.4    n    y    n    n    y  07/20/09 02:31 AM          180      14861.6   \n",
       "\n",
       "   NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0         93.29     0.03      15661        0.49           0.1  \n",
       "1         93.29     0.03      15661        0.49           0.1  \n",
       "2         93.29     0.03      15661        0.49           0.1  \n",
       "3         93.29     0.03      15661        0.49           0.1  \n",
       "4         93.29     0.03      15661        0.49           0.1  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", parse_dates=True, infer_datetime_format=True, \n",
    "            sep = None, encoding = \"latin-1\", engine = \"python\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plit data\n",
    "train, test = sk.cross_validation.train_test_split(data, train_size = 0.66)\n",
    "\n",
    "token_dict = {}\n",
    "\n",
    "# http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html\n",
    "# http://glowingpython.blogspot.de/2013/07/combining-scikit-learn-and-ntlk.html\n",
    "# http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "# https://stackoverflow.com/questions/10526579/use-scikit-learn-to-classify-into-multiple-categories\n",
    "# https://github.com/anuraagvak/IRE-PersonalityRecognition-Final/blob/master/ire_report.pdf\n",
    "# https://github.com/Charudatt89/Personality_Recognition/blob/master/22-9-PersonalityRecognition/Report/Report.pdf\n",
    "\n",
    "#classif = SklearnClassifier(LinearSVC())\n",
    "#classif.train(train)\n",
    "\n",
    "#fdist1 = n.FreqDist(list_of_rows_CD)\n",
    "#fdist1.most_common(5)\n",
    "#fdist1.hapaxes()\n",
    "#set(w.lower() for w in list_of_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all the strings for each author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02c37028a782cfda660c7243e45244bb</td>\n",
       "      <td>abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjkl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03133a828cd0cf52e3752813ce5d818f</td>\n",
       "      <td>Feels aweful right now. Why do you get sick li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03e6c4eca4269c183fa0e1780f73faba</td>\n",
       "      <td>\"I refuse to answer that question on the groun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06b055f8e2bca96496514891057913c3</td>\n",
       "      <td>is enjoying the cricket...comfy boxers and rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0724fe854bd455061ba84efecdeff469</td>\n",
       "      <td>Life truly has special ways to show us what we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  02c37028a782cfda660c7243e45244bb   \n",
       "1  03133a828cd0cf52e3752813ce5d818f   \n",
       "2  03e6c4eca4269c183fa0e1780f73faba   \n",
       "3  06b055f8e2bca96496514891057913c3   \n",
       "4  0724fe854bd455061ba84efecdeff469   \n",
       "\n",
       "                                              STATUS  \n",
       "0  abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjkl...  \n",
       "1  Feels aweful right now. Why do you get sick li...  \n",
       "2  \"I refuse to answer that question on the groun...  \n",
       "3  is enjoying the cricket...comfy boxers and rai...  \n",
       "4  Life truly has special ways to show us what we...  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset = train.groupby(['#AUTHID'])['STATUS'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "combined_dataset.to_csv(path_or_buf=\"oneBigString.csv\")\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many unique values are there in the first column ?\n",
    "#### Basically number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values = train[train.columns[0]]\n",
    "pd.Series.nunique(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e6cdef6f475cce3023c5b715f8c9f110    158\n",
       "6f2bebc01062eb8334dccba3e048fdb5    145\n",
       "527ed53d2ba3a3bc417b8402d5b2f556    140\n",
       "d7e500ad854a1b6ced39e53a525b8a6d    114\n",
       "0737e4e4980f56c9fb1cb5743001c917    113\n",
       "502db2fcfe26705ae16a46c5cb2ad2e5    112\n",
       "715c9eb832dc833a0b6409ddccd268b1    107\n",
       "f7456ac4e6b20911c40fdad18908a8d2    104\n",
       "b4a21c82de4011033c8ac67081ff939c    104\n",
       "b2be41464b53ffc6deae9536ddfd3aee    104\n",
       "c3f4b3e345cb6b032db2e0459d179db3     98\n",
       "0bfa3d952ffed50f25011b128e73a820     93\n",
       "dbdfbfda2a4205bd59b22758ceddd5af     87\n",
       "e4a512374eee079d2b8acc2ce69990d5     85\n",
       "d39c2b0fb2e50e37795fdbe3b8cd3792     83\n",
       "181962441153a36333f0c60701823412     78\n",
       "e465fadd8b30e8669f397e32e10f6cd0     72\n",
       "f2026b8cb48aff9af31577ecbfda5c38     71\n",
       "8d7faa6d7f104a6cb7c4a9e1c6310a15     71\n",
       "521896b01c1a506dc4404e600fa99c5b     69\n",
       "37195f370c3fd7486ccedb1519b026c2     68\n",
       "dba5f5266d03dd6d4db084ad7dbc683c     67\n",
       "c85845b82e705a32f674757d8912df23     67\n",
       "1695fbad7101e34d4cb4686a6770231f     66\n",
       "eb7f8081aa0bd4004f513d3299db9063     65\n",
       "4b8c9b247d45495cdb1ebf755fcec1f6     65\n",
       "b7b7764cfa1c523e4e93ab2a79a946c4     63\n",
       "c5d9ffcb242053b0abdebe0d684fea3a     62\n",
       "acfd53e1393633ae24f8c946d79a17d4     61\n",
       "cd99c28741e42fd9792616d3a4328f17     61\n",
       "                                   ... \n",
       "d38a81dfbfbbd5f2a2a0a03e9db304c6      3\n",
       "374035e42f4b691c2b9c7b915ef1aeae      2\n",
       "a7d9818f6165cd5c84b8578185ccd616      2\n",
       "da8787edc39ba38d6bc9f0d10e28aa0f      2\n",
       "325e62f4e7e4f64a03fcf831a8d80bf1      2\n",
       "740aa055e9dccaee874ab7ec7e499d8e      2\n",
       "3fe44fab3eb561ae418a22182ec75fad      2\n",
       "fbe5aa478508d1dc931427ade5d9e1b4      2\n",
       "785d03537a119e9decd7de253c74c306      2\n",
       "ddaed24e83f0f9958336b52cf7a89373      2\n",
       "43e9b5847ad2e94f53bcc9d826093a76      2\n",
       "ea28a927cb6663480ea33ca917c3c8ba      2\n",
       "69adae32cb076bf219e0d856ef233008      2\n",
       "da22dab36bb12fcf9ed1a437de278a2c      2\n",
       "deb899e426c1a5c66c24eeb0d7df6257      2\n",
       "baa58f6b30d937855af7bd7c22f0756b      2\n",
       "ac8bf16a381d07c01b11651994ecb746      1\n",
       "5532642937eb3497a43e15dbb23a9d2d      1\n",
       "8974aab97d9fc4e3a53ba126b5eedd81      1\n",
       "127d3a99f86b3ee848fd0449bec048fc      1\n",
       "789ce9b31990354f0a5a507347844dea      1\n",
       "c255a1cb2939ce6b4719a8a0cc085624      1\n",
       "7e0954e34b5af347696eb260230bccda      1\n",
       "eaf7165a60baa108b9db9508eb4d3cc8      1\n",
       "a764ca41dca158d7a191505dcc8ce47f      1\n",
       "a286b7286b1247d4a7851709e9f31e1e      1\n",
       "c723c2f329649b2af235fdefd1ca293c      1\n",
       "19c6d69f9f5acc1a43d6ac498085e69f      1\n",
       "cef8086fc8220dde9874948728787a2c      1\n",
       "341d74a026925b6a0bde7b58c519c414      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/21633580/pandas-counting-unique-values-in-a-dataframe\n",
    "pd.value_counts(unique_values.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our main column consists of the rows of sentences. One after another.\n",
    "#### Make all words lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjkl...\n",
      "1    feels aweful right now. why do you get sick li...\n",
      "2    \"i refuse to answer that question on the groun...\n",
      "3    is enjoying the cricket...comfy boxers and rai...\n",
      "4    life truly has special ways to show us what we...\n",
      "Name: STATUS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lines_of_combined_dataset = combined_dataset[\"STATUS\"].str.lower()\n",
    "print(lines_of_combined_dataset.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's split words for each sentence and make real sentences (python-usable objects) from rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "twt = n.TreebankWordTokenizer()\n",
    "english = n.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "# Create lists\n",
    "list_of_rows_CD = [l.split(\"\\n\")[0] for l in lines_of_combined_dataset]\n",
    "\n",
    "\n",
    "list_of_splitted_words_CD, list_of_sentences = [], []\n",
    "\n",
    "for k in list_of_rows_CD:\n",
    "    list_of_sentences.append(n.sent_tokenize(k))\n",
    "    \n",
    "for kCD in list_of_rows_CD:\n",
    "    list_of_splitted_words_CD.append(n.word_tokenize(kCD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ':', 'd', 'cartography+select', '.', 'topics', 'in', 'the', 'geography', 'of', 'china', '(', '?', '?', '?', '?', 'f', '?', 'add', '?', '...', ')', 'pa', '>', '<', 'dol', 'x', '2'], ['feels', 'aweful', 'right', 'now', '.', 'why', 'do', 'you', 'get', 'sick', 'like', 'this', 'on', 'weekends', 'only', '...', 'i', 'better', 'not', 'have', 'to', 'use', 'a', 'sick', 'day', 'tomorrow', 'or', 'i', 'will', 'be', 'pissed', '.', 'did', '*propname*', 'in', '16:37', ',', 'made', 'money', 'in', 'vegas', ',', 'and', 'just', 'had', 'his', '3rd', '10', 'hour', 'plus', 'day', 'this', 'month', '.', 'not', 'bad', '...', 'and', 'to', 'clarify', '*propname*', 'is', 'a', 'crossfit', 'workout', ',', '*propname*', 'get', 'your', 'head', 'out', 'of', 'the', 'gutter', 'did', '*propname*', 'in', '16:37', ',', 'made', 'money', 'in', 'vegas', ',', 'and', 'just', 'had', 'his', '3rd', '10', 'hour', 'plus', 'day', 'this', 'week', '.', 'life', 'is', 'good', 'did', '*propname*', 'in', '16:37', ',', 'made', 'money', 'in', 'vegas', ',', 'and', 'just', 'had', 'his', '3rd', '10', 'hour', 'plus', 'day', 'this', 'month', '.', 'not', 'bad', 'did', 'cindy', '30', 'times', 'in', '20', 'minutes', 'and', 'gi', 'jane', 'in', '12:11.', 'save', 'the', 'sex', 'joke', ',', 'its', 'crossfit', '.', '2010', 'crossfit', 'games', 'here', 'i', 'come', '!', '!', '!', '!']] \n",
      "\n",
      " ['abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjklzxcvbnm mnbvcxzlkjhgfdsapoiuytrewq dea dea dea :d cartography+select. topics in the geography of china     (????f?add?...) pa><dol x 2', 'feels aweful right now. why do you get sick like this on weekends only... i better not have to use a sick day tomorrow or i will be pissed. did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this month. not bad...and to clarify *propname* is a crossfit workout, *propname* get your head out of the gutter did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this week. life is good did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this month. not bad did cindy 30 times in 20 minutes and gi jane in 12:11. save the sex joke, its crossfit. 2010 crossfit games here i come!!!!'] \n",
      "\n",
      " [['abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjklzxcvbnm mnbvcxzlkjhgfdsapoiuytrewq dea dea dea :d cartography+select.', 'topics in the geography of china     (????f?add?...)', 'pa><dol x 2'], ['feels aweful right now.', 'why do you get sick like this on weekends only... i better not have to use a sick day tomorrow or i will be pissed.', 'did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this month.', 'not bad...and to clarify *propname* is a crossfit workout, *propname* get your head out of the gutter did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this week.', 'life is good did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this month.', 'not bad did cindy 30 times in 20 minutes and gi jane in 12:11. save the sex joke, its crossfit.', '2010 crossfit games here i come!!!', '!']]\n"
     ]
    }
   ],
   "source": [
    "print(list_of_splitted_words_CD[:2], \"\\n\\n\", list_of_rows_CD[:2], \"\\n\\n\", list_of_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 117596 tokens in 242 sentences\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/2058985/python-count-sub-lists-in-nested-list\n",
    "print(\"there are\", sum(len(x) for x in list_of_splitted_words_CD), \"tokens in\", len(list_of_sentences), \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjklzxcvbnm mnbvcxzlkjhgfdsapoiuytrewq dea dea dea :d cartography+select. topics in the geography of china     (????f?add?...) pa><dol x 2', 'feels aweful right now. why do you get sick like this on weekends only... i better not have to use a sick day tomorrow or i will be pissed. did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this month. not bad...and to clarify *propname* is a crossfit workout, *propname* get your head out of the gutter did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this week. life is good did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this month. not bad did cindy 30 times in 20 minutes and gi jane in 12:11. save the sex joke, its crossfit. 2010 crossfit games here i come!!!!'] \n",
      "\n",
      "[['abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjklzxcvbnm mnbvcxzlkjhgfdsapoiuytrewq dea dea dea :d cartography+select.', 'topics in the geography of china     (????f?add?...)', 'pa><dol x 2'], ['feels aweful right now.', 'why do you get sick like this on weekends only... i better not have to use a sick day tomorrow or i will be pissed.', 'did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this month.', 'not bad...and to clarify *propname* is a crossfit workout, *propname* get your head out of the gutter did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this week.', 'life is good did *propname* in 16:37, made money in vegas, and just had his 3rd 10 hour plus day this month.', 'not bad did cindy 30 times in 20 minutes and gi jane in 12:11. save the sex joke, its crossfit.', '2010 crossfit games here i come!!!', '!']] \n",
      "\n",
      "[['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ':', 'd', 'cartography+select', '.', 'topics', 'in', 'the', 'geography', 'of', 'china', '(', '?', '?', '?', '?', 'f', '?', 'add', '?', '...', ')', 'pa', '>', '<', 'dol', 'x', '2'], ['feels', 'aweful', 'right', 'now', '.', 'why', 'do', 'you', 'get', 'sick', 'like', 'this', 'on', 'weekends', 'only', '...', 'i', 'better', 'not', 'have', 'to', 'use', 'a', 'sick', 'day', 'tomorrow', 'or', 'i', 'will', 'be', 'pissed', '.', 'did', '*propname*', 'in', '16:37', ',', 'made', 'money', 'in', 'vegas', ',', 'and', 'just', 'had', 'his', '3rd', '10', 'hour', 'plus', 'day', 'this', 'month', '.', 'not', 'bad', '...', 'and', 'to', 'clarify', '*propname*', 'is', 'a', 'crossfit', 'workout', ',', '*propname*', 'get', 'your', 'head', 'out', 'of', 'the', 'gutter', 'did', '*propname*', 'in', '16:37', ',', 'made', 'money', 'in', 'vegas', ',', 'and', 'just', 'had', 'his', '3rd', '10', 'hour', 'plus', 'day', 'this', 'week', '.', 'life', 'is', 'good', 'did', '*propname*', 'in', '16:37', ',', 'made', 'money', 'in', 'vegas', ',', 'and', 'just', 'had', 'his', '3rd', '10', 'hour', 'plus', 'day', 'this', 'month', '.', 'not', 'bad', 'did', 'cindy', '30', 'times', 'in', '20', 'minutes', 'and', 'gi', 'jane', 'in', '12:11.', 'save', 'the', 'sex', 'joke', ',', 'its', 'crossfit', '.', '2010', 'crossfit', 'games', 'here', 'i', 'come', '!', '!', '!', '!']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list_of_rows_CD[:2], \"\\n\")\n",
    "print(list_of_sentences[:2],\"\\n\") \n",
    "print(list_of_splitted_words_CD[:2], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different tokenizers (will decide later on which one)\n",
    "\n",
    "Source: http://text-processing.com/demo/tokenize/\n",
    "\n",
    "\n",
    "<img src=\"dif_tokenizers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Delete stopwords from each row\n",
    "\n",
    "#### And store them again in each row (= nested list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stopwords.words(\"english\") \n",
    "# https://stackoverflow.com/questions/19249201/how-to-create-a-number-of-empty-nested-lists-in-python\n",
    "english_stops_list = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def delete_stopwords():\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    nest_list_without_stopwords = [[] for _ in range(len(list_of_splitted_words_CD))]\n",
    "    for sentence in list_of_splitted_words_CD: \n",
    "        for word in sentence:\n",
    "            if word not in english_stops:\n",
    "                nest_list_without_stopwords[list_of_splitted_words_CD.index(sentence)].append(word)\n",
    "    return nest_list_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ...], ['feels', 'aweful', 'right', 'now', '.', 'why', ...], ['``', 'i', 'refuse', 'to', 'answer', 'that', ...], ['is', 'enjoying', 'the', 'cricket', '...', 'comfy', ...], ['life', 'truly', 'has', 'special', 'ways', 'to', ...], ['just', 'booked', 'the', 'rest', 'of', 'my', ...], ...] \n",
      "\n",
      "[['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ...], ['feels', 'aweful', 'right', '.', 'get', 'sick', ...], ['``', 'refuse', 'answer', 'question', 'grounds', \"n't\", ...], ['enjoying', 'cricket', '...', 'comfy', 'boxers', 'rainy', ...], ['life', 'truly', 'special', 'ways', 'show', 'us', ...], ['booked', 'rest', 'flights', '!', '!', '!', ...], ...]\n"
     ]
    }
   ],
   "source": [
    "nest_list_without_stopwords = delete_stopwords()\n",
    "\n",
    "print(reprlib.repr(list_of_splitted_words_CD), \"\\n\")\n",
    "print(reprlib.repr(nest_list_without_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer \n",
    "\n",
    "##### https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
    "\n",
    "> Stemming is a technique to remove affixes from a word, ending up with the stem. For\n",
    "example, the stem of cooking is cook, and a good stemming algorithm knows that the ing\n",
    "suffix can be removed.\n",
    "\n",
    "\n",
    "### Lemmatization \n",
    "is very similar to stemming, but is more akin to synonym replacement. A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always left with a valid word that means the same thing. However, the word you end up with can be completely different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \n",
      " [['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ...], ['feels', 'aweful', 'right', '.', 'get', 'sick', ...], ['``', 'refuse', 'answer', 'question', 'grounds', \"n't\", ...], ['enjoying', 'cricket', '...', 'comfy', 'boxers', 'rainy', ...], ['life', 'truly', 'special', 'ways', 'show', 'us', ...], ['booked', 'rest', 'flights', '!', '!', '!', ...]] \n",
      "\n",
      "SnowballStemmer \n",
      " [['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ...], ['feel', 'awe', 'right', '.', 'get', 'sick', ...], ['``', 'refus', 'answer', 'question', 'ground', \"n't\", ...], ['enjoy', 'cricket', '...', 'comfi', 'boxer', 'raini', ...], ['life', 'truli', 'special', 'way', 'show', 'us', ...], ['book', 'rest', 'flight', '!', '!', '!', ...], ...] \n",
      "\n",
      "Porter \n",
      " [['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ...], ['feel', 'awe', 'right', '.', 'get', 'sick', ...], ['``', 'refus', 'answer', 'question', 'ground', \"n't\", ...], ['enjoy', 'cricket', '...', 'comfi', 'boxer', 'raini', ...], ['life', 'truli', 'special', 'way', 'show', 'us', ...], ['book', 'rest', 'flight', '!', '!', '!', ...], ...] \n",
      "\n",
      "Lemmatizer \n",
      " [['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ...], ['feel', 'aweful', 'right', '.', 'get', 'sick', ...], ['``', 'refuse', 'answer', 'question', 'ground', \"n't\", ...], ['enjoy', 'cricket', '...', 'comfy', 'boxers', 'rainy', ...], ['life', 'truly', 'special', 'ways', 'show', 'us', ...], ['book', 'rest', 'flight', '!', '!', '!', ...], ...] \n",
      "\n",
      "LancasterStemmer \n",
      " [['abcdefghijklmnopqrstuvwxys', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ...], ['feel', 'aw', 'right', '.', 'get', 'sick', ...], ['``', 'refus', 'answ', 'quest', 'ground', \"n't\", ...], ['enjoy', 'cricket', '...', 'comfy', 'box', 'rainy', ...], ['lif', 'tru', 'spec', 'way', 'show', 'us', ...], ['book', 'rest', 'flight', '!', '!', '!', ...], ...] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Huge nested lists\n",
    "# http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet\n",
    "\n",
    "nest_list_without_stopwords_ss = [[] for _ in range(len(nest_list_without_stopwords))]\n",
    "nest_list_without_stopwords_pp = [[] for _ in range(len(nest_list_without_stopwords))]\n",
    "nest_list_without_stopwords_ls = [[] for _ in range(len(nest_list_without_stopwords))]\n",
    "nest_list_without_stopwords_lm = [[] for _ in range(len(nest_list_without_stopwords))]\n",
    "\n",
    "ss = n.stem.SnowballStemmer(\"english\")\n",
    "pp = n.stem.PorterStemmer()\n",
    "ls = n.stem.LancasterStemmer()\n",
    "lm = n.stem.WordNetLemmatizer()\n",
    "\n",
    "for sentence in nest_list_without_stopwords: \n",
    "    for word in sentence:\n",
    "        nest_list_without_stopwords_ss[nest_list_without_stopwords.index(sentence)].append(ss.stem(word))\n",
    "        nest_list_without_stopwords_pp[nest_list_without_stopwords.index(sentence)].append(pp.stem(word))\n",
    "        nest_list_without_stopwords_lm[nest_list_without_stopwords.index(sentence)].append(lm.lemmatize(word, pos=\"v\"))\n",
    "        nest_list_without_stopwords_ls[nest_list_without_stopwords.index(sentence)].append(ls.stem(word))\n",
    "\n",
    "print(\"Original \\n\", reprlib.repr(nest_list_without_stopwords[:6]), \"\\n\")\n",
    "print(\"SnowballStemmer \\n\", reprlib.repr(nest_list_without_stopwords_ss), \"\\n\")\n",
    "print(\"Porter \\n\", reprlib.repr(nest_list_without_stopwords_pp), \"\\n\")\n",
    "print(\"Lemmatizer \\n\", reprlib.repr(nest_list_without_stopwords_lm), \"\\n\")\n",
    "print(\"LancasterStemmer \\n\", reprlib.repr(nest_list_without_stopwords_ls), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdfword'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# book 38\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):        \n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "    \n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "        \n",
    "    def delete_stupid_chars(self, word):\n",
    "        \"\"\"\n",
    "        http://stackoverflow.com/a/3874768\n",
    "        used above\n",
    "        \"\"\"\n",
    "        replaced_word = self.replace(word)\n",
    "        rem = \"!?#.,();:'[].,//``...~<>$%^&*-_-=+\"\n",
    "        return replaced_word.translate(str.maketrans(dict.fromkeys(rem)))\n",
    "\n",
    "replacer = RepeatReplacer()\n",
    "replacer.delete_stupid_chars(\"!?sdf,word!??)()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def delete_repChars():\n",
    "    nest_list_without_stopwords_lm_repchars = [[] for _ in range(len(nest_list_without_stopwords_lm))]\n",
    "    for sentence in nest_list_without_stopwords_lm:\n",
    "        for word in sentence:\n",
    "            nest_list_without_stopwords_lm_repchars[nest_list_without_stopwords_lm.index(sentence)].append(replacer.delete_stupid_chars(word))\n",
    "    return nest_list_without_stopwords_lm_repchars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ...], ['feel', 'aweful', 'right', '', 'get', 'sick', ...], ['', 'refuse', 'answer', 'question', 'ground', 'nt', ...], ['enjoy', 'cricket', '', 'comfy', 'boxers', 'rainy', ...], ['life', 'truly', 'special', 'ways', 'show', 'us', ...], ['book', 'rest', 'flight', '', '', '', ...], ...]\n"
     ]
    }
   ],
   "source": [
    "nest_list_without_stopwords_lm_repchars = delete_repChars()\n",
    "print(reprlib.repr(nest_list_without_stopwords_lm_repchars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[nest_list_without_stopwords_lm_repchars for nest_list_without_stopwords_lm_repchars in nest_list_without_stopwords_lm_repchars if nest_list_without_stopwords_lm_repchars]\n",
    "#    nest_list_without_stopwords_lm_repchars_as = [[] for _ in range(len(nest_list_without_stopwords_lm_repchars))]\n",
    "#    for sentence in nest_list_without_stopwords_lm_repchars:\n",
    "#        nest_list_without_stopwords_lm_repchars_as[nest_list_without_stopwords_lm_repchars.index(sentence)].append(list(filter(None, sentence)))   \n",
    "#https://stackoverflow.com/questions/973568/convert-nested-lists-to-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def delete_empty_strings():\n",
    "    nest = [[] for _ in range(len(nest_list_without_stopwords_lm_repchars))]\n",
    "    for sentence in nest_list_without_stopwords_lm_repchars: \n",
    "        for word in sentence:\n",
    "            if word != '':\n",
    "                nest[nest_list_without_stopwords_lm_repchars.index(sentence)].append(word)\n",
    "    return nest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nest = delete_empty_strings()\n",
    "outlst = [' '.join([str(c) for c in hm]) for hm in nest]\n",
    "\n",
    "#print(outlst)\n",
    "# print(len(nest_list_without_stopwords_lm_repchars[61]), \"\\n\", len(nest_list_without_stopwords_lm[61]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devil: 4.09516617054\n"
     ]
    }
   ],
   "source": [
    "#http://aylien.com/web-summit-2015-tweets-part1\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_features = 10000)\n",
    "vz = vectorizer.fit_transform(outlst)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print(\"devil: \" + str(tfidf[\"devil\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tagged words from all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjklzxcvbnm mnbvcxzlkjhgfdsapoiuytrewq dea dea dea :d cartography+select. topics in the geography of china     (????f?add?...) pa><dol x 2\n",
      "['abcdefghijklmnopqrstuvwxyz', 'qwertyuiopasdfghjklzxcvbnm', 'mnbvcxzlkjhgfdsapoiuytrewq', 'dea', 'dea', 'dea', ':', 'd', 'cartography+select', '.', 'topics', 'in', 'the', 'geography', 'of', 'china', '(', '?', '?', '?', '?', 'f', '?', 'add', '?', '...', ')', 'pa', '>', '<', 'dol', 'x', '2']\n",
      "['abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjklzxcvbnm mnbvcxzlkjhgfdsapoiuytrewq dea dea dea :d cartography+select.', 'topics in the geography of china     (????f?add?...)', 'pa><dol x 2'] \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isdigit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-332-52977952b70e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtag_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_splitted_words_CD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# print(reprlib.repr(tagged_words()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \"\"\"\n\u001b[0;32m    110\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en-ptb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'!HYPHEN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'!YEAR'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'isdigit'"
     ]
    }
   ],
   "source": [
    "#print(reprlib.repr(nest_list_without_stopwords_lm))\n",
    "\n",
    "print(list_of_rows_CD[0])\n",
    "print(list_of_splitted_words_CD[0])\n",
    "print(list_of_sentences[0], \"\\n\")\n",
    "\n",
    "tag_words = n.pos_tag(list_of_splitted_words_CD)\n",
    "print(tag_words, \"\\n\")\n",
    "# print(reprlib.repr(tagged_words()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nest_list_tagged_words = [[] for _ in range(len(nest_list_without_stopwords_lm_repchars))]\n",
    "\n",
    "#def tagged_words():\n",
    "#    for sentence in nest_list_without_stopwords_lm_repchars:\n",
    "#        for words in sentence:\n",
    "#            nest_list_tagged_words[nest_list_without_stopwords_lm_repchars.index(sentence)].append(n.pos_tag(words))\n",
    "#    return nest_list_tagged_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram features\n",
    "\n",
    "Use -a for code analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 9; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-334-60c1a386ae7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-334-60c1a386ae7b>\u001b[0m in \u001b[0;36mword_fea\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mword_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 9; 2 is required"
     ]
    }
   ],
   "source": [
    "def word_fea(words):\n",
    "    return dict((word, True))\n",
    "word_fea(outlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bigram collocation\n",
    "# https://github.com/neotenic/cancer/blob/master/nltk.ipynb\n",
    "def bigram_features(words, score_fn=BAM.chi_sq): \n",
    "    bg_finder = BigramCollocationFinder.from_words(words) \n",
    "    bigrams = bg_finder.nbest(score_fn, 100000) \n",
    "    return dict((bg, True) for bg in chain(words, bigrams)) \n",
    "\n",
    "#bigram_features(outlst, score_fn=BAM.chi_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-337-e7dd2442d0e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"%.3f\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;31m# the label and featurename.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "cl = n.NaiveBayesClassifier.train(train)\n",
    "print(n.classify.accuracy(cl, test),\"%.3f\")\n",
    "cl.show_most_informative_features(40)\n",
    "cl.prob_classify(featurize(name)) #"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
