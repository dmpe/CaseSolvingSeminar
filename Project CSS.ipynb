{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk as n\n",
    "import pandas as pd\n",
    "import nltk.tokenize\n",
    "# n.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", parse_dates=True, infer_datetime_format=True, \n",
    "            sep = None, encoding = \"latin-1\", engine = \"python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/15/09 01:15 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/22/09 04:48 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/20/09 02:31 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65     3  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65     3  3.15  3.25   \n",
       "2  is sore and wants the knot of muscles at the b...  2.65     3  3.15  3.25   \n",
       "3         likes how the day sounds in this new song.  2.65     3  3.15  3.25   \n",
       "4                                        is home. <3  2.65     3  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  BETWEENNESS  \\\n",
       "0   4.4    n    y    n    n    y  06/19/09 03:21 PM          180      14861.6   \n",
       "1   4.4    n    y    n    n    y  07/02/09 08:41 AM          180      14861.6   \n",
       "2   4.4    n    y    n    n    y  06/15/09 01:15 PM          180      14861.6   \n",
       "3   4.4    n    y    n    n    y  06/22/09 04:48 AM          180      14861.6   \n",
       "4   4.4    n    y    n    n    y  07/20/09 02:31 AM          180      14861.6   \n",
       "\n",
       "   NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0         93.29     0.03      15661        0.49           0.1  \n",
       "1         93.29     0.03      15661        0.49           0.1  \n",
       "2         93.29     0.03      15661        0.49           0.1  \n",
       "3         93.29     0.03      15661        0.49           0.1  \n",
       "4         93.29     0.03      15661        0.49           0.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line = data[\"STATUS\"]\n",
    "#line\n",
    "\n",
    "\n",
    "#[n.word_tokenize(t) for t in n.sent_tokenize(line)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split words and sentences of our main column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 9917 tokens and there are  9917\n",
      "[['likes the sound of thunder.'], [\"is so sleepy it's not even funny that's she can't get to sleep.\"], ['is sore and wants the knot of muscles at the base of her neck to stop hurting.', \"On the other hand, YAY I'M IN ILLINOIS!\", '<3'], ['likes how the day sounds in this new song.'], ['is home.', '<3'], ['www.thejokerblogs.com']]\n"
     ]
    }
   ],
   "source": [
    "list_of_rows = []\n",
    "words = []\n",
    "sentences = []\n",
    "\n",
    "list_of_rows = [l.split(\"\\n\")[0] for l in line ]\n",
    "\n",
    "\n",
    "for k in list_of_rows:\n",
    "    words.append(n.word_tokenize(k))\n",
    "    sentences.append(n.sent_tokenize(k))\n",
    "    \n",
    "print(\"there are\", len(words), \"tokens and there are \", len(sentences))\n",
    "\n",
    "print(sentences[:6])\n",
    "\n",
    "#set(w.lower() for w in list_of_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 9879 samples and 9917 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('black', 4),\n",
       " ('*PROPNAME*', 4),\n",
       " ('Merry Christmas everyone!', 3),\n",
       " ('Happy New Year!', 3),\n",
       " ('No one should die because they cannot afford health care, and no one should go broke because they get sick. If you agree, please post this as your status for the rest of the day.',\n",
       "  3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1 = n.FreqDist(list_of_rows)\n",
    "print(fdist1)\n",
    "\n",
    "fdist1.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fdist1.hapaxes()\n",
    "list = [l.split(\"\\n\")[0] for l in line ]\n",
    "\n",
    "#tokens = n.word_tokenize(list)\n",
    "#tokens\n",
    "\n",
    "#snet = n.sent_tokenize(list_of_rows)\n",
    "#len(snet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"is so sleepy it's not even funny that's she can't get to sleep.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_rows[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is so sleepy it's not even funny that's she can't get to sleep.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('is', 'VBZ'),\n",
       " ('so', 'RB'),\n",
       " ('sleepy', 'JJ'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('even', 'RB'),\n",
       " ('funny', 'VBP'),\n",
       " ('that', 'DT'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('she', 'PRP'),\n",
       " ('ca', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('get', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('sleep', 'VB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.head()\n",
    "print(line[1])\n",
    "\n",
    "tag_words = n.pos_tag(words[1])\n",
    "tag_words\n",
    "\n",
    "\n",
    "\n",
    "#sent_words = n.pos_tag_sents(sentences)\n",
    "#sent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'s\": True,\n",
       " '.': True,\n",
       " 'ca': True,\n",
       " 'even': True,\n",
       " 'funny': True,\n",
       " 'get': True,\n",
       " 'is': True,\n",
       " 'it': True,\n",
       " \"n't\": True,\n",
       " 'not': True,\n",
       " 'she': True,\n",
       " 'sleep': True,\n",
       " 'sleepy': True,\n",
       " 'so': True,\n",
       " 'that': True,\n",
       " 'to': True}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "train, test = sk.cross_validation.train_test_split(data, train_size = 0.8)\n",
    "\n",
    "\n",
    "def word_fea(words):\n",
    "    return dict((word, True) for word in words)\n",
    "\n",
    "#Unigram features\n",
    "word_fea(words[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5129ca8d6d84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtext4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"words\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbigram_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBAM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchi_sq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/text.py\u001b[0m in \u001b[0;36mcollocations\u001b[1;34m(self, num, window_size)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mfinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_word_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mignored_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[0mbigram_measures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBigramAssocMeasures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collocations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_measures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihood_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[0mcolloc_strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collocations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolloc_strings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"; \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/nltk/collocations.py\u001b[0m in \u001b[0;36mnbest\u001b[1;34m(self, score_fn, n)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnbest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;34m\"\"\"Returns the top n ngrams when scored by the given function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mabove_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "# Bigram collocation\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from itertools import chain\n",
    "from nltk.book import text4\n",
    "\n",
    "#text4.collocations(\"words\")\n",
    "\n",
    "def bigram_features(words, score_fn=BAM.chi_sq): \n",
    "    bg_finder = BigramCollocationFinder.from_words(words) \n",
    "    bigrams = bg_finder.nbest(score_fn, 100000) \n",
    "    return dict((bg, True) for bg in chain(words, bigrams)) \n",
    "\n",
    "\n",
    "bigram_features(words[1], score_fn=BAM.chi_sq)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
