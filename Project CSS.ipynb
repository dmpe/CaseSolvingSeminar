{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/IPython/extensions/cythonmagic.py:21: UserWarning: The Cython magic has been moved to the Cython package\n",
      "  warnings.warn(\"\"\"The Cython magic has been moved to the Cython package\"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, reprlib, sys\n",
    "\n",
    "%load_ext cython\n",
    "%load_ext cythonmagic\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem, nltk.book\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# n.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/15/09 01:15 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/22/09 04:48 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/20/09 02:31 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65     3  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65     3  3.15  3.25   \n",
       "2  is sore and wants the knot of muscles at the b...  2.65     3  3.15  3.25   \n",
       "3         likes how the day sounds in this new song.  2.65     3  3.15  3.25   \n",
       "4                                        is home. <3  2.65     3  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  BETWEENNESS  \\\n",
       "0   4.4    n    y    n    n    y  06/19/09 03:21 PM          180      14861.6   \n",
       "1   4.4    n    y    n    n    y  07/02/09 08:41 AM          180      14861.6   \n",
       "2   4.4    n    y    n    n    y  06/15/09 01:15 PM          180      14861.6   \n",
       "3   4.4    n    y    n    n    y  06/22/09 04:48 AM          180      14861.6   \n",
       "4   4.4    n    y    n    n    y  07/20/09 02:31 AM          180      14861.6   \n",
       "\n",
       "   NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0         93.29     0.03      15661        0.49           0.1  \n",
       "1         93.29     0.03      15661        0.49           0.1  \n",
       "2         93.29     0.03      15661        0.49           0.1  \n",
       "3         93.29     0.03      15661        0.49           0.1  \n",
       "4         93.29     0.03      15661        0.49           0.1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", parse_dates=True, infer_datetime_format=True, \n",
    "            sep = None, encoding = \"latin-1\", engine = \"python\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.354760</td>\n",
       "      <td>2.609453</td>\n",
       "      <td>3.616643</td>\n",
       "      <td>3.474201</td>\n",
       "      <td>4.130386</td>\n",
       "      <td>429.377120</td>\n",
       "      <td>135425.315359</td>\n",
       "      <td>94.665170</td>\n",
       "      <td>3.154012</td>\n",
       "      <td>137642.476201</td>\n",
       "      <td>0.489920</td>\n",
       "      <td>0.128821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.857578</td>\n",
       "      <td>0.760248</td>\n",
       "      <td>0.682485</td>\n",
       "      <td>0.737215</td>\n",
       "      <td>0.585672</td>\n",
       "      <td>428.760382</td>\n",
       "      <td>199433.803497</td>\n",
       "      <td>5.506696</td>\n",
       "      <td>311.073343</td>\n",
       "      <td>201392.066555</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>0.106063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.330000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.710000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>16902.200000</td>\n",
       "      <td>93.770000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>17982.000000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.400000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>47166.900000</td>\n",
       "      <td>96.440000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>48683.000000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.550000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>196606.000000</td>\n",
       "      <td>97.880000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>198186.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>29724.900000</td>\n",
       "      <td>1251780.000000</td>\n",
       "      <td>99.820000</td>\n",
       "      <td>30978.000000</td>\n",
       "      <td>1263790.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sEXT         sNEU         sAGR         sCON         sOPN  \\\n",
       "count  9917.000000  9917.000000  9917.000000  9917.000000  9917.000000   \n",
       "mean      3.354760     2.609453     3.616643     3.474201     4.130386   \n",
       "std       0.857578     0.760248     0.682485     0.737215     0.585672   \n",
       "min       1.330000     1.250000     1.650000     1.450000     2.250000   \n",
       "25%       2.710000     2.000000     3.140000     3.000000     3.750000   \n",
       "50%       3.400000     2.600000     3.650000     3.400000     4.250000   \n",
       "75%       4.000000     3.050000     4.150000     4.000000     4.550000   \n",
       "max       5.000000     4.750000     5.000000     5.000000     5.000000   \n",
       "\n",
       "        NETWORKSIZE     BETWEENNESS  NBETWEENNESS       DENSITY  \\\n",
       "count   9917.000000     9917.000000   9917.000000   9917.000000   \n",
       "mean     429.377120   135425.315359     94.665170      3.154012   \n",
       "std      428.760382   199433.803497      5.506696    311.073343   \n",
       "min       24.000000       93.250000      0.040000      0.000000   \n",
       "25%      196.000000    16902.200000     93.770000      0.010000   \n",
       "50%      317.000000    47166.900000     96.440000      0.020000   \n",
       "75%      633.000000   196606.000000     97.880000      0.030000   \n",
       "max    29724.900000  1251780.000000     99.820000  30978.000000   \n",
       "\n",
       "            BROKERAGE   NBROKERAGE  TRANSITIVITY  \n",
       "count     9917.000000  9917.000000   9916.000000  \n",
       "mean    137642.476201     0.489920      0.128821  \n",
       "std     201392.066555     0.011908      0.106063  \n",
       "min          0.490000     0.180000      0.000000  \n",
       "25%      17982.000000     0.490000      0.060000  \n",
       "50%      48683.000000     0.490000      0.090000  \n",
       "75%     198186.000000     0.500000      0.170000  \n",
       "max    1263790.000000     0.500000      0.630000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all the strings for each author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00419a4c96b32cd63b2c7196da761274</td>\n",
       "      <td>back in cali!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02c37028a782cfda660c7243e45244bb</td>\n",
       "      <td>Supervisor: *PROPNAME* (second preference) Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03133a828cd0cf52e3752813ce5d818f</td>\n",
       "      <td>Did Cindy 30 times in 20 minutes and GI jane i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03e6c4eca4269c183fa0e1780f73faba</td>\n",
       "      <td>\"Those who criticize our generation forget who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06b055f8e2bca96496514891057913c3</td>\n",
       "      <td>is enjoying the cricket...comfy boxers and rai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  00419a4c96b32cd63b2c7196da761274   \n",
       "1  02c37028a782cfda660c7243e45244bb   \n",
       "2  03133a828cd0cf52e3752813ce5d818f   \n",
       "3  03e6c4eca4269c183fa0e1780f73faba   \n",
       "4  06b055f8e2bca96496514891057913c3   \n",
       "\n",
       "                                              STATUS  \n",
       "0                                    back in cali!!!  \n",
       "1  Supervisor: *PROPNAME* (second preference) Res...  \n",
       "2  Did Cindy 30 times in 20 minutes and GI jane i...  \n",
       "3  \"Those who criticize our generation forget who...  \n",
       "4  is enjoying the cricket...comfy boxers and rai...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset = data.groupby(['#AUTHID'])['STATUS'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "combined_dataset.to_csv(path_or_buf=\"oneBigString.csv\")\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many unique values are there in the first column ?\n",
    "#### Basically number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values = data[data.columns[0]]\n",
    "pd.Series.nunique(unique_values)\n",
    "#len(set(unique_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e6cdef6f475cce3023c5b715f8c9f110    223\n",
       "6f2bebc01062eb8334dccba3e048fdb5    219\n",
       "527ed53d2ba3a3bc417b8402d5b2f556    194\n",
       "d7e500ad854a1b6ced39e53a525b8a6d    184\n",
       "0737e4e4980f56c9fb1cb5743001c917    172\n",
       "502db2fcfe26705ae16a46c5cb2ad2e5    165\n",
       "b4a21c82de4011033c8ac67081ff939c    162\n",
       "b2be41464b53ffc6deae9536ddfd3aee    159\n",
       "c3f4b3e345cb6b032db2e0459d179db3    153\n",
       "715c9eb832dc833a0b6409ddccd268b1    151\n",
       "f7456ac4e6b20911c40fdad18908a8d2    150\n",
       "0bfa3d952ffed50f25011b128e73a820    141\n",
       "dbdfbfda2a4205bd59b22758ceddd5af    126\n",
       "e4a512374eee079d2b8acc2ce69990d5    126\n",
       "f2026b8cb48aff9af31577ecbfda5c38    123\n",
       "e465fadd8b30e8669f397e32e10f6cd0    118\n",
       "181962441153a36333f0c60701823412    114\n",
       "8d7faa6d7f104a6cb7c4a9e1c6310a15    114\n",
       "d39c2b0fb2e50e37795fdbe3b8cd3792    113\n",
       "eb7f8081aa0bd4004f513d3299db9063    107\n",
       "521896b01c1a506dc4404e600fa99c5b    104\n",
       "dba5f5266d03dd6d4db084ad7dbc683c    102\n",
       "b7b7764cfa1c523e4e93ab2a79a946c4    102\n",
       "3d7847b1c33b5f5811208b4aa1a7ffbd    101\n",
       "c5d9ffcb242053b0abdebe0d684fea3a     99\n",
       "37195f370c3fd7486ccedb1519b026c2     98\n",
       "4b8c9b247d45495cdb1ebf755fcec1f6     95\n",
       "8f9d4ed5d16ed1a67d734196d29d1f6b     95\n",
       "cd99c28741e42fd9792616d3a4328f17     93\n",
       "526ac2635cb3f5e0ee5d7424e83e9107     93\n",
       "                                   ... \n",
       "d38a81dfbfbbd5f2a2a0a03e9db304c6      3\n",
       "a7d9818f6165cd5c84b8578185ccd616      3\n",
       "341d74a026925b6a0bde7b58c519c414      3\n",
       "43e9b5847ad2e94f53bcc9d826093a76      3\n",
       "aec40862b2a12be50b4d04347985b54d      3\n",
       "ddaed24e83f0f9958336b52cf7a89373      2\n",
       "da22dab36bb12fcf9ed1a437de278a2c      2\n",
       "6980ce18350d98916f56c95b4dc4496d      2\n",
       "3fe44fab3eb561ae418a22182ec75fad      2\n",
       "deb899e426c1a5c66c24eeb0d7df6257      2\n",
       "7e0954e34b5af347696eb260230bccda      2\n",
       "69adae32cb076bf219e0d856ef233008      2\n",
       "eaf7165a60baa108b9db9508eb4d3cc8      2\n",
       "325e62f4e7e4f64a03fcf831a8d80bf1      2\n",
       "7226dfa2f0bde9e94bac3c83a3b1ab7a      2\n",
       "740aa055e9dccaee874ab7ec7e499d8e      2\n",
       "789ce9b31990354f0a5a507347844dea      2\n",
       "ea28a927cb6663480ea33ca917c3c8ba      2\n",
       "a286b7286b1247d4a7851709e9f31e1e      1\n",
       "00419a4c96b32cd63b2c7196da761274      1\n",
       "a764ca41dca158d7a191505dcc8ce47f      1\n",
       "127d3a99f86b3ee848fd0449bec048fc      1\n",
       "22d1f7b24168528163c515b1c96a879c      1\n",
       "c255a1cb2939ce6b4719a8a0cc085624      1\n",
       "8974aab97d9fc4e3a53ba126b5eedd81      1\n",
       "35efb99775d5ee7e83cf7912591984d5      1\n",
       "ac8bf16a381d07c01b11651994ecb746      1\n",
       "19c6d69f9f5acc1a43d6ac498085e69f      1\n",
       "f6cb2eff458f065858363e86515beaab      1\n",
       "5532642937eb3497a43e15dbb23a9d2d      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/21633580/pandas-counting-unique-values-in-a-dataframe\n",
    "pd.value_counts(unique_values.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our main column consists of the rows of sentences. One after another.\n",
    "#### Make all words lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          likes the sound of thunder.\n",
      "1    is so sleepy it's not even funny that's she ca...\n",
      "2    is sore and wants the knot of muscles at the b...\n",
      "3           likes how the day sounds in this new song.\n",
      "4                                          is home. <3\n",
      "Name: STATUS, dtype: object\n",
      "0                                      back in cali!!!\n",
      "1    supervisor: *propname* (second preference) res...\n",
      "2    did cindy 30 times in 20 minutes and gi jane i...\n",
      "3    \"those who criticize our generation forget who...\n",
      "4    is enjoying the cricket...comfy boxers and rai...\n",
      "Name: STATUS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lines_of_data = data[\"STATUS\"].str.lower()\n",
    "print(lines_of_data.head(n=5))\n",
    "\n",
    "lines_of_combined_dataset = combined_dataset[\"STATUS\"].str.lower()\n",
    "print(lines_of_combined_dataset.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's split words for each sentence and make real sentences (python-usable objects) from rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "twt = n.TreebankWordTokenizer()\n",
    "english = n.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "# Create lists\n",
    "list_of_rows = [l.split(\"\\n\")[0] for l in lines_of_data]\n",
    "list_of_rows_CD = [l.split(\"\\n\")[0] for l in lines_of_combined_dataset]\n",
    "\n",
    "\n",
    "list_of_splitted_words, list_of_splitted_words_CD, list_of_splitted_words_twt, list_of_splitted_words_faster = [], [], [], []\n",
    "list_of_sentences, list_of_sentences_twt = [], []\n",
    "\n",
    "for k in list_of_rows:\n",
    "    list_of_splitted_words.append(n.word_tokenize(k))\n",
    "    list_of_splitted_words_faster.append(english.tokenize(k))\n",
    "    list_of_sentences.append(n.sent_tokenize(k))\n",
    "    \n",
    "\n",
    "for kCD in list_of_rows_CD:\n",
    "    list_of_splitted_words_CD.append(n.word_tokenize(kCD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['likes the sound of thunder.', \"is so sleepy it's not even funny that's she can't get to sleep.\"] \n",
      " ['back in cali!!!', 'supervisor: *propname* (second preference) research area: regional economic integration (fifth prefernece) \\x85\\x85  tentative examination schedule, semester 1, 2009//10. . abcdefghijklmnopqrstuvwxyz qwertyuiopasdfghjklzxcvbnm mnbvcxzlkjhgfdsapoiuytrewq pa><dol x 2 cartography+select. topics in the geography of china     (????f?add?...) dea dea dea :d']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_rows[:2], \"\\n\", list_of_rows_CD[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 177474 tokens and there are 9917 sentences \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/2058985/python-count-sub-lists-in-nested-list\n",
    "print(\"there are\", sum(len(x) for x in list_of_splitted_words), \"tokens and there are\", \n",
    "      len(list_of_sentences), \"sentences \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They both will be same\n",
      "['likes the sound of thunder.', \"is so sleepy it's not even funny that's she can't get to sleep.\"] \n",
      "\n",
      "[['likes the sound of thunder.'], [\"is so sleepy it's not even funny that's she can't get to sleep.\"]]\n",
      "[['likes', 'the', 'sound', 'of', 'thunder', '.'], ['is', 'so', 'sleepy', 'it', \"'s\", 'not', 'even', 'funny', 'that', \"'s\", 'she', 'ca', \"n't\", 'get', 'to', 'sleep', '.']]\n"
     ]
    }
   ],
   "source": [
    "#' This is the method that is invoked by word_tokenize(). It assumes that the text has already \n",
    "#' been segmented into sentences, e.g. using sent_tokenize().\n",
    "\n",
    "print(\"They both will be same\")\n",
    "print(list_of_rows[:2], \"\\n\")\n",
    "print(list_of_sentences[:2]) #, \" \\n\", list_of_sentences_twt[:2], \"\\n\")\n",
    "print(list_of_splitted_words[:2]) #, \" \\n\", list_of_splitted_words_twt[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different tokenizers (will decide later on which one)\n",
    "\n",
    "Source: http://text-processing.com/demo/tokenize/\n",
    "\n",
    "\n",
    "<img src=\"dif_tokenizers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Delete stopwords from each row\n",
    "\n",
    "#### And store them again in each row (= nested list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['likes', 'the', 'sound', 'of', 'thunder', '.'], ['is', 'so', 'sleepy', 'it', \"'s\", 'not', 'even', 'funny', 'that', \"'s\", 'she', 'ca', \"n't\", 'get', 'to', 'sleep', '.']]\n"
     ]
    }
   ],
   "source": [
    "# stopwords.words(\"english\") \n",
    "# https://stackoverflow.com/questions/19249201/how-to-create-a-number-of-empty-nested-lists-in-python\n",
    "\n",
    "print(list_of_splitted_words[:2])\n",
    "english_stops_list = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def delete_stopwords():\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    nest_list_without_stopwords = [[] for _ in range(len(list_of_splitted_words))]\n",
    "    for sentence in list_of_splitted_words: \n",
    "        for word in sentence:\n",
    "            if word not in english_stops:\n",
    "                nest_list_without_stopwords[list_of_splitted_words.index(sentence)].append(word)\n",
    "    return nest_list_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['likes', 'sound', 'thunder', '.'], ['sleepy', \"'s\", 'even', 'funny', \"'s\", 'ca', ...], ['sore', 'wants', 'knot', 'muscles', 'base', 'neck', ...], ['likes', 'day', 'sounds', 'new', 'song', '.'], ['home', '.', '<', '3'], ['www.thejokerblogs.com'], ...]\n"
     ]
    }
   ],
   "source": [
    "nest_list_without_stopwords = delete_stopwords()     \n",
    "print(reprlib.repr(nest_list_without_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer \n",
    "\n",
    "##### https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
    "\n",
    "> Stemming is a technique to remove affixes from a word, ending up with the stem. For\n",
    "example, the stem of cooking is cook, and a good stemming algorithm knows that the ing\n",
    "suffix can be removed.\n",
    "\n",
    "\n",
    "### Lemmatization \n",
    "is very similar to stemming, but is more akin to synonym replacement. A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always left with a valid word that means the same thing. However, the word you end up with can be completely different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \n",
      " [['likes', 'sound', 'thunder', '.'], ['sleepy', \"'s\", 'even', 'funny', \"'s\", 'ca', ...], ['sore', 'wants', 'knot', 'muscles', 'base', 'neck', ...], ['likes', 'day', 'sounds', 'new', 'song', '.'], ['home', '.', '<', '3'], ['www.thejokerblogs.com']] \n",
      "\n",
      "SnowballStemmer \n",
      " [['like', 'sound', 'thunder', '.'], ['sleepi', \"'s\", 'even', 'funni', \"'s\", 'ca', ...], ['sore', 'want', 'knot', 'muscl', 'base', 'neck', ...], ['like', 'day', 'sound', 'new', 'song', '.'], ['home', '.', '<', '3'], ['www.thejokerblogs.com'], ...] \n",
      "\n",
      "Porter \n",
      " [['like', 'sound', 'thunder', '.'], ['sleepi', \"'s\", 'even', 'funni', \"'s\", 'ca', ...], ['sore', 'want', 'knot', 'muscl', 'base', 'neck', ...], ['like', 'day', 'sound', 'new', 'song', '.'], ['home', '.', '<', '3'], ['www.thejokerblogs.com'], ...] \n",
      "\n",
      "Lemmatizer \n",
      " [['like', 'sound', 'thunder', '.'], ['sleepy', \"'s\", 'even', 'funny', \"'s\", 'ca', ...], ['sore', 'want', 'knot', 'muscle', 'base', 'neck', ...], ['like', 'day', 'sound', 'new', 'song', '.'], ['home', '.', '<', '3'], ['www.thejokerblogs.com'], ...] \n",
      "\n",
      "LancasterStemmer \n",
      " [['lik', 'sound', 'thund', '.'], ['sleepy', \"'s\", 'ev', 'funny', \"'s\", 'ca', ...], ['sor', 'want', 'knot', 'musc', 'bas', 'neck', ...], ['lik', 'day', 'sound', 'new', 'song', '.'], ['hom', '.', '<', '3'], ['www.thejokerblogs.com'], ...] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Huge nested lists\n",
    "# http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet\n",
    "\n",
    "nest_list_without_stopwords_ss = [[] for _ in range(len(nest_list_without_stopwords))]\n",
    "nest_list_without_stopwords_pp = [[] for _ in range(len(nest_list_without_stopwords))]\n",
    "nest_list_without_stopwords_ls = [[] for _ in range(len(nest_list_without_stopwords))]\n",
    "nest_list_without_stopwords_lm = [[] for _ in range(len(nest_list_without_stopwords))]\n",
    "\n",
    "ss = n.stem.SnowballStemmer(\"english\")\n",
    "pp = n.stem.PorterStemmer()\n",
    "ls = n.stem.LancasterStemmer()\n",
    "lm = n.stem.WordNetLemmatizer()\n",
    "\n",
    "for sentence in nest_list_without_stopwords: \n",
    "    for word in sentence:\n",
    "        nest_list_without_stopwords_ss[nest_list_without_stopwords.index(sentence)].append(ss.stem(word))\n",
    "        nest_list_without_stopwords_pp[nest_list_without_stopwords.index(sentence)].append(pp.stem(word))\n",
    "        nest_list_without_stopwords_lm[nest_list_without_stopwords.index(sentence)].append(lm.lemmatize(word, pos=\"v\"))\n",
    "        nest_list_without_stopwords_ls[nest_list_without_stopwords.index(sentence)].append(ls.stem(word))\n",
    "\n",
    "print(\"Original \\n\", reprlib.repr(nest_list_without_stopwords[:6]), \"\\n\")\n",
    "print(\"SnowballStemmer \\n\", reprlib.repr(nest_list_without_stopwords_ss), \"\\n\")\n",
    "print(\"Porter \\n\", reprlib.repr(nest_list_without_stopwords_pp), \"\\n\")\n",
    "print(\"Lemmatizer \\n\", reprlib.repr(nest_list_without_stopwords_lm), \"\\n\")\n",
    "print(\"LancasterStemmer \\n\", reprlib.repr(nest_list_without_stopwords_ls), \"\\n\")\n",
    "\n",
    "# var1, var2, var3, var4 = [], [], [], []\n",
    "# for word in nest_list_without_stopwords[9]: \n",
    "#     var1.append(ss.stem(word))\n",
    "#     var2.append(pp.stem(word))\n",
    "#     var3.append(lm.lemmatize(word, pos=\"v\")) # or n -> makes difference!\n",
    "#     var4.append(ls.stem(word))\n",
    "# print(var1, \"\\n\", var2, \"\\n\", var3, \"\\n\", var4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complete', 'love', 'blackberry', 'mom', 'surprise', '.', 'yes', '.', 'excite', ',', 'cap', 'completely', 'necessary', '.', '<', '3', '<', '3', '<', '3', '<', '3', '<', '3', '<', '3']\n"
     ]
    }
   ],
   "source": [
    "# book 38\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "    \n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "        \n",
    "print(nest_list_without_stopwords_lm[61])\n",
    "replacer = RepeatReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def delete_repChars():\n",
    "    nest_list_without_stopwords_lm_repchars = [[] for _ in range(len(nest_list_without_stopwords_lm))]\n",
    "    for sentence in nest_list_without_stopwords_lm:\n",
    "        for word in sentence:\n",
    "            nest_list_without_stopwords_lm_repchars[nest_list_without_stopwords_lm.index(sentence)].append(replacer.replace(word))\n",
    "    return nest_list_without_stopwords_lm_repchars\n",
    "\n",
    "print(reprlib.repr(delete_repChars()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nest_list_without_stopwords_lm_repchars = delete_repChars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tagged words from all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(reprlib.repr(nest_list_without_stopwords_lm))\n",
    "\n",
    "print(list_of_rows[0])\n",
    "print(list_of_splitted_words[0])\n",
    "print(list_of_sentences[0], \"\\n\")\n",
    "len(list_of_splitted_words)\n",
    "\n",
    "tag_words = n.pos_tag(list_of_splitted_words[0])\n",
    "print(tag_words, \"\\n\")\n",
    "# print(reprlib.repr(tagged_words()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nest_list_tagged_words = [[] for _ in range(len(nest_list_without_stopwords_lm_repchars))]\n",
    "\n",
    "#def tagged_words():\n",
    "#    for sentence in nest_list_without_stopwords_lm_repchars:\n",
    "#        for words in sentence:\n",
    "#            nest_list_tagged_words[nest_list_without_stopwords_lm_repchars.index(sentence)].append(n.pos_tag(words))\n",
    "#    return nest_list_tagged_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram features\n",
    "\n",
    "Use -a for code analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_fea(words):\n",
    "    for word in words:\n",
    "        return dict((word, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_fea(nest_list_without_stopwords_lm[9547])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bigram collocation\n",
    "# https://github.com/neotenic/cancer/blob/master/nltk.ipynb\n",
    "def bigram_features(words, score_fn=BAM.chi_sq): \n",
    "    bg_finder = BigramCollocationFinder.from_words(words) \n",
    "    bigrams = bg_finder.nbest(score_fn, 100000) \n",
    "    return dict((bg, True) for bg in chain(words, bigrams)) \n",
    "\n",
    "\n",
    "bigram_features(list_of_splitted_words[1], score_fn=BAM.chi_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plit data\n",
    "train, test = sk.cross_validation.train_test_split(data, train_size = 0.8)\n",
    "\n",
    "token_dict = {}\n",
    "# tfidf = TfidfVectorizer(input=nest_list_without_stopwords_lm_repchars[0])\n",
    "# tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "# http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html\n",
    "# http://glowingpython.blogspot.de/2013/07/combining-scikit-learn-and-ntlk.html\n",
    "# http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "# https://stackoverflow.com/questions/10526579/use-scikit-learn-to-classify-into-multiple-categories\n",
    "# https://github.com/anuraagvak/IRE-PersonalityRecognition-Final/blob/master/ire_report.pdf\n",
    "# https://github.com/Charudatt89/Personality_Recognition/blob/master/22-9-PersonalityRecognition/Report/Report.pdf\n",
    "\n",
    "classif = SklearnClassifier(LinearSVC())\n",
    "classif.train(train)\n",
    "\n",
    "fdist1 = n.FreqDist(list_of_rows)\n",
    "#fdist1.most_common(5)\n",
    "#fdist1.hapaxes()\n",
    "#set(w.lower() for w in list_of_rows)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
