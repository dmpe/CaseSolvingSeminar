{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# n.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/15/09 01:15 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/22/09 04:48 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/20/09 02:31 AM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65     3  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65     3  3.15  3.25   \n",
       "2  is sore and wants the knot of muscles at the b...  2.65     3  3.15  3.25   \n",
       "3         likes how the day sounds in this new song.  2.65     3  3.15  3.25   \n",
       "4                                        is home. <3  2.65     3  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  BETWEENNESS  \\\n",
       "0   4.4    n    y    n    n    y  06/19/09 03:21 PM          180      14861.6   \n",
       "1   4.4    n    y    n    n    y  07/02/09 08:41 AM          180      14861.6   \n",
       "2   4.4    n    y    n    n    y  06/15/09 01:15 PM          180      14861.6   \n",
       "3   4.4    n    y    n    n    y  06/22/09 04:48 AM          180      14861.6   \n",
       "4   4.4    n    y    n    n    y  07/20/09 02:31 AM          180      14861.6   \n",
       "\n",
       "   NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0         93.29     0.03      15661        0.49           0.1  \n",
       "1         93.29     0.03      15661        0.49           0.1  \n",
       "2         93.29     0.03      15661        0.49           0.1  \n",
       "3         93.29     0.03      15661        0.49           0.1  \n",
       "4         93.29     0.03      15661        0.49           0.1  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", parse_dates=True, infer_datetime_format=True, \n",
    "            sep = None, encoding = \"latin-1\", engine = \"python\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9917.000000</td>\n",
       "      <td>9916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.354760</td>\n",
       "      <td>2.609453</td>\n",
       "      <td>3.616643</td>\n",
       "      <td>3.474201</td>\n",
       "      <td>4.130386</td>\n",
       "      <td>429.377120</td>\n",
       "      <td>135425.315359</td>\n",
       "      <td>94.665170</td>\n",
       "      <td>3.154012</td>\n",
       "      <td>137642.476201</td>\n",
       "      <td>0.489920</td>\n",
       "      <td>0.128821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.857578</td>\n",
       "      <td>0.760248</td>\n",
       "      <td>0.682485</td>\n",
       "      <td>0.737215</td>\n",
       "      <td>0.585672</td>\n",
       "      <td>428.760382</td>\n",
       "      <td>199433.803497</td>\n",
       "      <td>5.506696</td>\n",
       "      <td>311.073343</td>\n",
       "      <td>201392.066555</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>0.106063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.330000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>93.250000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.710000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>16902.200000</td>\n",
       "      <td>93.770000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>17982.000000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.400000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>47166.900000</td>\n",
       "      <td>96.440000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>48683.000000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.550000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>196606.000000</td>\n",
       "      <td>97.880000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>198186.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>29724.900000</td>\n",
       "      <td>1251780.000000</td>\n",
       "      <td>99.820000</td>\n",
       "      <td>30978.000000</td>\n",
       "      <td>1263790.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sEXT         sNEU         sAGR         sCON         sOPN  \\\n",
       "count  9917.000000  9917.000000  9917.000000  9917.000000  9917.000000   \n",
       "mean      3.354760     2.609453     3.616643     3.474201     4.130386   \n",
       "std       0.857578     0.760248     0.682485     0.737215     0.585672   \n",
       "min       1.330000     1.250000     1.650000     1.450000     2.250000   \n",
       "25%       2.710000     2.000000     3.140000     3.000000     3.750000   \n",
       "50%       3.400000     2.600000     3.650000     3.400000     4.250000   \n",
       "75%       4.000000     3.050000     4.150000     4.000000     4.550000   \n",
       "max       5.000000     4.750000     5.000000     5.000000     5.000000   \n",
       "\n",
       "        NETWORKSIZE     BETWEENNESS  NBETWEENNESS       DENSITY  \\\n",
       "count   9917.000000     9917.000000   9917.000000   9917.000000   \n",
       "mean     429.377120   135425.315359     94.665170      3.154012   \n",
       "std      428.760382   199433.803497      5.506696    311.073343   \n",
       "min       24.000000       93.250000      0.040000      0.000000   \n",
       "25%      196.000000    16902.200000     93.770000      0.010000   \n",
       "50%      317.000000    47166.900000     96.440000      0.020000   \n",
       "75%      633.000000   196606.000000     97.880000      0.030000   \n",
       "max    29724.900000  1251780.000000     99.820000  30978.000000   \n",
       "\n",
       "            BROKERAGE   NBROKERAGE  TRANSITIVITY  \n",
       "count     9917.000000  9917.000000   9916.000000  \n",
       "mean    137642.476201     0.489920      0.128821  \n",
       "std     201392.066555     0.011908      0.106063  \n",
       "min          0.490000     0.180000      0.000000  \n",
       "25%      17982.000000     0.490000      0.060000  \n",
       "50%      48683.000000     0.490000      0.090000  \n",
       "75%     198186.000000     0.500000      0.170000  \n",
       "max    1263790.000000     0.500000      0.630000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many unique values are there in the first column ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e6cdef6f475cce3023c5b715f8c9f110    223\n",
       "6f2bebc01062eb8334dccba3e048fdb5    219\n",
       "527ed53d2ba3a3bc417b8402d5b2f556    194\n",
       "d7e500ad854a1b6ced39e53a525b8a6d    184\n",
       "0737e4e4980f56c9fb1cb5743001c917    172\n",
       "502db2fcfe26705ae16a46c5cb2ad2e5    165\n",
       "b4a21c82de4011033c8ac67081ff939c    162\n",
       "b2be41464b53ffc6deae9536ddfd3aee    159\n",
       "c3f4b3e345cb6b032db2e0459d179db3    153\n",
       "715c9eb832dc833a0b6409ddccd268b1    151\n",
       "f7456ac4e6b20911c40fdad18908a8d2    150\n",
       "0bfa3d952ffed50f25011b128e73a820    141\n",
       "e4a512374eee079d2b8acc2ce69990d5    126\n",
       "dbdfbfda2a4205bd59b22758ceddd5af    126\n",
       "f2026b8cb48aff9af31577ecbfda5c38    123\n",
       "e465fadd8b30e8669f397e32e10f6cd0    118\n",
       "181962441153a36333f0c60701823412    114\n",
       "8d7faa6d7f104a6cb7c4a9e1c6310a15    114\n",
       "d39c2b0fb2e50e37795fdbe3b8cd3792    113\n",
       "eb7f8081aa0bd4004f513d3299db9063    107\n",
       "521896b01c1a506dc4404e600fa99c5b    104\n",
       "dba5f5266d03dd6d4db084ad7dbc683c    102\n",
       "b7b7764cfa1c523e4e93ab2a79a946c4    102\n",
       "3d7847b1c33b5f5811208b4aa1a7ffbd    101\n",
       "c5d9ffcb242053b0abdebe0d684fea3a     99\n",
       "37195f370c3fd7486ccedb1519b026c2     98\n",
       "4b8c9b247d45495cdb1ebf755fcec1f6     95\n",
       "8f9d4ed5d16ed1a67d734196d29d1f6b     95\n",
       "acfd53e1393633ae24f8c946d79a17d4     93\n",
       "cd99c28741e42fd9792616d3a4328f17     93\n",
       "                                   ... \n",
       "cef8086fc8220dde9874948728787a2c      3\n",
       "aec40862b2a12be50b4d04347985b54d      3\n",
       "c723c2f329649b2af235fdefd1ca293c      3\n",
       "fbe5aa478508d1dc931427ade5d9e1b4      3\n",
       "341d74a026925b6a0bde7b58c519c414      3\n",
       "6980ce18350d98916f56c95b4dc4496d      2\n",
       "ddaed24e83f0f9958336b52cf7a89373      2\n",
       "740aa055e9dccaee874ab7ec7e499d8e      2\n",
       "3fe44fab3eb561ae418a22182ec75fad      2\n",
       "325e62f4e7e4f64a03fcf831a8d80bf1      2\n",
       "7226dfa2f0bde9e94bac3c83a3b1ab7a      2\n",
       "eaf7165a60baa108b9db9508eb4d3cc8      2\n",
       "7e0954e34b5af347696eb260230bccda      2\n",
       "789ce9b31990354f0a5a507347844dea      2\n",
       "da22dab36bb12fcf9ed1a437de278a2c      2\n",
       "ea28a927cb6663480ea33ca917c3c8ba      2\n",
       "deb899e426c1a5c66c24eeb0d7df6257      2\n",
       "69adae32cb076bf219e0d856ef233008      2\n",
       "22d1f7b24168528163c515b1c96a879c      1\n",
       "127d3a99f86b3ee848fd0449bec048fc      1\n",
       "8974aab97d9fc4e3a53ba126b5eedd81      1\n",
       "a764ca41dca158d7a191505dcc8ce47f      1\n",
       "ac8bf16a381d07c01b11651994ecb746      1\n",
       "00419a4c96b32cd63b2c7196da761274      1\n",
       "35efb99775d5ee7e83cf7912591984d5      1\n",
       "5532642937eb3497a43e15dbb23a9d2d      1\n",
       "19c6d69f9f5acc1a43d6ac498085e69f      1\n",
       "a286b7286b1247d4a7851709e9f31e1e      1\n",
       "f6cb2eff458f065858363e86515beaab      1\n",
       "c255a1cb2939ce6b4719a8a0cc085624      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/21633580/pandas-counting-unique-values-in-a-dataframe\n",
    "unique_values = data[data.columns[0]]\n",
    "pd.value_counts(unique_values.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our main column consists of the rows of sentences. One after another.\n",
    "#### Make all words lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          likes the sound of thunder.\n",
       "1    is so sleepy it's not even funny that's she ca...\n",
       "2    is sore and wants the knot of muscles at the b...\n",
       "3           likes how the day sounds in this new song.\n",
       "4                                          is home. <3\n",
       "Name: STATUS, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = data[\"STATUS\"]\n",
    "line = line.str.lower()\n",
    "line.head(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's split words for each sentence and make real sentences (python-usable objects) from rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 177474 tokens and there are 9917 sentences \n",
      "\n",
      "They both will be same\n",
      "['likes the sound of thunder.', \"is so sleepy it's not even funny that's she can't get to sleep.\"] \n",
      "\n",
      "[['likes the sound of thunder.'], [\"is so sleepy it's not even funny that's she can't get to sleep.\"]]  \n",
      " [['likes', 'the', 'sound', 'of', 'thunder', '.'], ['is', 'so', 'sleepy', 'it', \"'s\", 'not', 'even', 'funny', 'that', \"'s\", 'she', 'ca', \"n't\", 'get', 'to', 'sleep', '.']] \n",
      "\n",
      "WORDS -------- \n",
      "\n",
      "[['likes', 'the', 'sound', 'of', 'thunder', '.'], ['is', 'so', 'sleepy', 'it', \"'s\", 'not', 'even', 'funny', 'that', \"'s\", 'she', 'ca', \"n't\", 'get', 'to', 'sleep', '.']]  \n",
      " [['likes', 'the', 'sound', 'of', 'thunder', '.'], ['is', 'so', 'sleepy', 'it', \"'s\", 'not', 'even', 'funny', 'that', \"'s\", 'she', 'ca', \"n't\", 'get', 'to', 'sleep', '.']]\n"
     ]
    }
   ],
   "source": [
    "# http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\n",
    "twt = n.TreebankWordTokenizer()\n",
    "english = n.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "list_of_rows = []\n",
    "list_of_splitted_words, list_of_splitted_words_twt, list_of_splitted_words_faster = [], [], []\n",
    "list_of_sentences, list_of_sentences_twt = [], []\n",
    "\n",
    "\n",
    "list_of_rows = [l.split(\"\\n\")[0] for l in line ]\n",
    "\n",
    "for k in list_of_rows:\n",
    "    list_of_splitted_words.append(n.word_tokenize(k))\n",
    "    list_of_splitted_words_twt.append(twt.tokenize(k))\n",
    "    list_of_splitted_words_faster.append(english.tokenize(k))\n",
    "    list_of_sentences.append(n.sent_tokenize(k))\n",
    "    list_of_sentences_twt.append(twt.tokenize(k))\n",
    "\n",
    "    \n",
    "# https://stackoverflow.com/questions/2058985/python-count-sub-lists-in-nested-list\n",
    "print(\"there are\", sum(len(x) for x in list_of_splitted_words), \"tokens and there are\", \n",
    "      len(list_of_sentences), \"sentences \\n\")\n",
    "\n",
    "#' This is the method that is invoked by word_tokenize(). It assumes that the text has already \n",
    "#' been segmented into sentences, e.g. using sent_tokenize().\n",
    "\n",
    "print(\"They both will be same\")\n",
    "\n",
    "print(list_of_rows[:2], \"\\n\")\n",
    "\n",
    "print(list_of_sentences[:2], \" \\n\", list_of_sentences_twt[:2], \"\\n\")\n",
    "\n",
    "print(\"WORDS -------- \\n\")\n",
    "print(list_of_splitted_words[:2], \" \\n\", list_of_splitted_words_twt[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different tokenizers (will decide later on which one)\n",
    "\n",
    "Source: http://text-processing.com/demo/tokenize/\n",
    "\n",
    "\n",
    "<img src=\"dif_tokenizers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Delete stopwords from each row\n",
    "\n",
    "#### And store them again in each row (= nested list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['likes', 'the', 'sound', 'of', 'thunder', '.'], ['is', 'so', 'sleepy', 'it', \"'s\", 'not', 'even', 'funny', 'that', \"'s\", 'she', 'ca', \"n't\", 'get', 'to', 'sleep', '.']]\n"
     ]
    }
   ],
   "source": [
    "# stopwords.words(\"english\")\n",
    "# https://stackoverflow.com/questions/19249201/how-to-create-a-number-of-empty-nested-lists-in-python\n",
    "\n",
    "print(list_of_splitted_words[:2])\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "#english_stops_list = list(english_stops)\n",
    "\n",
    "nest_list_without_stopwords = [[] for _ in range(len(list_of_splitted_words))]\n",
    "\n",
    "for sentence in list_of_splitted_words: \n",
    "    for word in sentence:\n",
    "        if word not in english_stops:\n",
    "            nest_list_without_stopwords[list_of_splitted_words.index(sentence)].append(word)\n",
    "#print(nest_list_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer \n",
    "\n",
    "##### https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
    "\n",
    "> Stemming is a technique to remove affixes from a word, ending up with the stem. For\n",
    "example, the stem of cooking is cook, and a good stemming algorithm knows that the ing\n",
    "suffix can be removed.\n",
    "\n",
    "\n",
    "### Lemmatization \n",
    "is very similar to stemming, but is more akin to synonym replacement. A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always left with a valid word that means the same thing. However, the word you end up with can be completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['celebr', 'new', 'haircut', 'listen', 'swinger', 'music', 'general', 'look', 'like', 'doofus', '.'] \n",
      " ['celebr', 'new', 'haircut', 'listen', 'swinger', 'music', 'gener', 'look', 'like', 'doofu', '.'] \n",
      " ['celebrate', 'new', 'haircut', 'listen', 'swinger', 'music', 'generally', 'look', 'like', 'doofus', '.'] \n",
      " ['celebr', 'new', 'haircut', 'list', 'swing', 'mus', 'gen', 'look', 'lik', 'doof', '.']\n"
     ]
    }
   ],
   "source": [
    "ss = n.stem.SnowballStemmer(\"english\")\n",
    "pp = n.stem.PorterStemmer()\n",
    "ls = n.stem.LancasterStemmer()\n",
    "lm = n.stem.WordNetLemmatizer()\n",
    "\n",
    "var1, var2, var3, var4 = [], [], [], []\n",
    "\n",
    "for word in nest_list_without_stopwords[9]: \n",
    "    var1.append(ss.stem(word))\n",
    "    var2.append(pp.stem(word))\n",
    "    var3.append(lm.lemmatize(word, pos=\"v\")) # or n -> makes difference!\n",
    "    var4.append(ls.stem(word))\n",
    "print(var1, \"\\n\", var2, \"\\n\", var3, \"\\n\", var4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complete', 'love', 'blackberry', 'mom', 'surprised', '.', 'yes', '.', 'excited', ',', 'caps', 'completely', 'necessary', '.', '<', '3', '<', '3', '<', '3', '<', '3', '<', '3', '<', '3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nest_list_without_stopwords[0]'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# book 38\n",
    "class RepeatReplacer(object):\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "    \n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "    \n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "        \n",
    "print(nest_list_without_stopwords[61])\n",
    "replacer = RepeatReplacer()\n",
    "replacer.replace(\"nest_list_without_stopwords[0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tagged words from all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes the sound of thunder.\n",
      "['likes', 'the', 'sound', 'of', 'thunder', '.']\n",
      "['likes the sound of thunder.'] \n",
      "\n",
      "[('likes', 'VBZ'), ('the', 'DT'), ('sound', 'NN'), ('of', 'IN'), ('thunder', 'NN'), ('.', '.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(list_of_rows[0])\n",
    "print(list_of_splitted_words[0])\n",
    "print(list_of_sentences[0], \"\\n\")\n",
    "\n",
    "tag_words = n.pos_tag(list_of_splitted_words[0])\n",
    "print(tag_words, \"\\n\")\n",
    "\n",
    "# code below creates what is needed later\n",
    "te = []\n",
    "#for sentence in list_of_sentences:\n",
    "#    for words in list_of_splitted_words:\n",
    "        #te = n.pos_tag(words)\n",
    "        #print(te)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'s\": True,\n",
       " '.': True,\n",
       " 'ca': True,\n",
       " 'even': True,\n",
       " 'funny': True,\n",
       " 'get': True,\n",
       " 'is': True,\n",
       " 'it': True,\n",
       " \"n't\": True,\n",
       " 'not': True,\n",
       " 'she': True,\n",
       " 'sleep': True,\n",
       " 'sleepy': True,\n",
       " 'so': True,\n",
       " 'that': True,\n",
       " 'to': True}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Unigram features\n",
    "def word_fea(words):\n",
    "    return dict((word, True) for word in words)\n",
    "\n",
    "word_fea(list_of_splitted_words[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ca': True,\n",
       " 'get': True,\n",
       " ('not', 'even'): True,\n",
       " 'even': True,\n",
       " \"'s\": True,\n",
       " (\"n't\", 'get'): True,\n",
       " '.': True,\n",
       " ('so', 'sleepy'): True,\n",
       " 'is': True,\n",
       " 'so': True,\n",
       " 'sleepy': True,\n",
       " 'sleep': True,\n",
       " \"n't\": True,\n",
       " 'to': True,\n",
       " (\"'s\", 'not'): True,\n",
       " ('even', 'funny'): True,\n",
       " ('is', 'so'): True,\n",
       " ('ca', \"n't\"): True,\n",
       " ('get', 'to'): True,\n",
       " (\"'s\", 'she'): True,\n",
       " ('sleep', '.'): True,\n",
       " 'funny': True,\n",
       " 'it': True,\n",
       " ('it', \"'s\"): True,\n",
       " ('that', \"'s\"): True,\n",
       " 'she': True,\n",
       " ('funny', 'that'): True,\n",
       " 'that': True,\n",
       " 'not': True,\n",
       " ('sleepy', 'it'): True,\n",
       " ('she', 'ca'): True,\n",
       " ('to', 'sleep'): True}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram collocation\n",
    "# https://github.com/neotenic/cancer/blob/master/nltk.ipynb\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from itertools import chain\n",
    "from nltk.book import text4\n",
    "\n",
    "def bigram_features(words, score_fn=BAM.chi_sq): \n",
    "    bg_finder = BigramCollocationFinder.from_words(words) \n",
    "    bigrams = bg_finder.nbest(score_fn, 100000) \n",
    "    return dict((bg, True) for bg in chain(words, bigrams)) \n",
    "\n",
    "\n",
    "bigram_features(list_of_splitted_words[1], score_fn=BAM.chi_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plit data\n",
    "train, test = sk.cross_validation.train_test_split(data, train_size = 0.8)\n",
    "\n",
    "fdist1 = n.FreqDist(list_of_rows)\n",
    "#fdist1.most_common(5)\n",
    "#fdist1.hapaxes()\n",
    "#set(w.lower() for w in list_of_rows)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
