{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/12761510/python-how-can-i-calculate-the-average-word-length-in-a-sentence-using-the-spl\n",
    "\n",
    "https://pythonprogramming.net/combine-classifier-algorithms-nltk-tutorial/?completed=/sklearn-scikit-learn-nltk-tutorial/\n",
    "\n",
    "http://streamhacker.com/2012/11/22/text-classification-sentiment-analysis-nltk-scikitlearn/\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sample-pipeline-for-text-feature-extraction-and-evaluation\n",
    "\n",
    "http://www.aicbt.com/authorship-attribution/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/IPython/extensions/cythonmagic.py:21: UserWarning: The Cython magic has been moved to the Cython package\n",
      "  warnings.warn(\"\"\"The Cython magic has been moved to the Cython package\"\"\")\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/qda.py:4: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, reprlib, sys, csv, logging, subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import *\n",
    "import random as ran\n",
    "from pandas_confusion import BinaryConfusionMatrix, Backend\n",
    "\n",
    "ran.seed(5125)\n",
    "\n",
    "from pprint import *\n",
    "from time import *\n",
    "from rpy2 import *\n",
    "\n",
    "%load_ext cython\n",
    "%load_ext cythonmagic\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from pandas_confusion import *\n",
    "from scipy.cluster.vq import *\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem\n",
    "from nltk.corpus import *\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from nltk.probability import *\n",
    "from nltk.classify.scikitlearn import *\n",
    "from nltk.tag.sequential import *\n",
    "from nltk.tag.util import *\n",
    "from nltk.tag import *\n",
    "\n",
    "from sklearn_pandas import *\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.datasets import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.grid_search import *\n",
    "from sklearn.base import *\n",
    "from sklearn.datasets.twenty_newsgroups import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_extraction import *\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute R script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['Rscript', 'lexical_features.R']' returned non-zero exit status 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f8dea7210b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath2script\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniversal_newlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/python3.4/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['Rscript', 'lexical_features.R']' returned non-zero exit status 2"
     ]
    }
   ],
   "source": [
    "# http://www.mango-solutions.com/wp/2015/10/integrating-python-and-r-part-ii-executing-r-from-python-and-vice-versa/\n",
    "command = 'Rscript'\n",
    "path2script = 'lexical_features.R'\n",
    "\n",
    "# Build subprocess command\n",
    "cmd = [command, path2script]\n",
    "\n",
    "subprocess.check_output(cmd, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID                       STATUS  sEXT  sNEU  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4  likes the sound of thunder.  2.65     3   \n",
       "\n",
       "   sAGR  sCON  sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  \\\n",
       "0  3.15  3.25   4.4    n    y    n    n    y  06/19/09 03:21 PM          180   \n",
       "\n",
       "   BETWEENNESS  NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0      14861.6         93.29     0.03      15661        0.49           0.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../raw_data/data.csv\", parse_dates=True, infer_datetime_format=True, \n",
    "            sep = None, encoding = \"latin-1\", engine = \"python\")\n",
    "data.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utf8 = pd.read_csv(\"../raw_data/data_utf8.csv\", parse_dates=True, infer_datetime_format=True)\n",
    "data_utf8.head(n=1)\n",
    "data_utf8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "word = \"likes the sound of thunder.\"\n",
    "print(sum(c != ' ' for c in word))\n",
    "# data[\"Lenght_of_Rows_withSpaces\"] = np.nan\n",
    "# data[\"Lenght_of_Rows_withoutSpaces\"] = np.nan\n",
    "# data[\"Word_Lenght\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'n': \"no\", 'y': \"yes\"}\n",
    "\n",
    "# http://stackoverflow.com/a/17702781\n",
    "data = data.replace(d)\n",
    "\n",
    "neu = data[[\"#AUTHID\",\"STATUS\",\"cNEU\"]]\n",
    "ext = data[[\"#AUTHID\",\"STATUS\",\"cEXT\"]]\n",
    "agr = data[[\"#AUTHID\",\"STATUS\",\"cAGR\"]]\n",
    "con = data[[\"#AUTHID\",\"STATUS\",\"cCON\"]]\n",
    "opn = data[[\"#AUTHID\",\"STATUS\",\"cOPN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no                                  4093\n",
       "yes                                 2453\n",
       "6f2bebc01062eb8334dccba3e048fdb5     146\n",
       "e6cdef6f475cce3023c5b715f8c9f110     136\n",
       "502db2fcfe26705ae16a46c5cb2ad2e5     121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neu1, test_neu1 = sk.cross_validation.train_test_split(neu, train_size = 0.66, stratify= neu[\"cNEU\"])\n",
    "train_neu1.stack().value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no                                  4096\n",
       "yes                                 2449\n",
       "6f2bebc01062eb8334dccba3e048fdb5     145\n",
       "e6cdef6f475cce3023c5b715f8c9f110     144\n",
       "527ed53d2ba3a3bc417b8402d5b2f556     132\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neu, test_neu = sk.cross_validation.train_test_split(neu, train_size = 0.66)\n",
    "train_neu.stack().value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "train_neu, test_neu = sk.cross_validation.train_test_split(neu, train_size = 0.66, stratify= neu[\"cNEU\"])\n",
    "train_ext, test_ext = sk.cross_validation.train_test_split(ext, train_size = 0.66, stratify= ext[\"cEXT\"])\n",
    "train_agr, test_agr = sk.cross_validation.train_test_split(agr, train_size = 0.66, stratify= agr[\"cAGR\"])\n",
    "train_con, test_con = sk.cross_validation.train_test_split(con, train_size = 0.66, stratify= con[\"cCON\"])\n",
    "train_opn, test_opn = sk.cross_validation.train_test_split(opn, train_size = 0.66, stratify= opn[\"cOPN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "classifierNB = Pipeline([\n",
    "    ('vectorizer_tfidf', TfidfVectorizer(ngram_range = (1,2))),\n",
    "    ('nb', LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  49 tasks       | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 199 tasks       | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks       | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 799 tasks       | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1249 tasks       | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1799 tasks       | elapsed: 26.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2449 tasks       | elapsed: 34.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3199 tasks       | elapsed: 44.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rbf kernel SVC: 0.582 (+/- 0.01)\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__n_iter': (10, 50, 80),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__max_features': (None, 5000, 10000, 50000),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 1152 candidates, totalling 3456 fits\n",
      "done in 2911.556s\n",
      "\n",
      "Best score: 0.607\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tclf__n_iter: 10\n",
      "\tclf__penalty: 'l2'\n",
      "\ttfidf__norm: 'l1'\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__max_features: None\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed: 48.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(neu[\"STATUS\"], neu[\"cNEU\"])\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c4c0e16b0bda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mfeature_sets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mfeature_sets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSyntacticFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"STATUS\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'neu' is not defined"
     ]
    }
   ],
   "source": [
    "def SyntacticFeatures(chapters):\n",
    "    \"\"\"\n",
    "    Extract feature vector for part of speech frequencies\n",
    "    \"\"\"\n",
    "    list_of_rows_CD = [l.split(\"\\n\")[0] for l in chapters]\n",
    "    def token_to_pos(ch):\n",
    "        tokens = n.word_tokenize(ch)\n",
    "        #print(tokens)\n",
    "        return [p[1] for p in n.pos_tag(tokens)]\n",
    "\n",
    "    chapters_pos = [token_to_pos(kCD) for kCD in list_of_rows_CD]\n",
    "    print(chapters_pos)\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[ch.count(pos) for pos in pos_list]\n",
    "                           for ch in chapters_pos]).astype(np.float64)\n",
    "    # normalise by dividing each row by number of tokens in the chapter\n",
    "    fvs_syntax /= np.c_[np.array([len(ch) for ch in chapters_pos])]\n",
    "    return fvs_syntax\n",
    "\n",
    "feature_sets = None\n",
    "feature_sets = list(SyntacticFeatures(neu[\"STATUS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LexicalFeatures(chapters):\n",
    "    sentence_tokenizer = n.data.load('tokenizers/punkt/english.pickle')\n",
    "    word_tokenizer = n.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    # create feature vectors\n",
    "    num_chapters = len(chapters)\n",
    "    fvs_lexical = np.zeros((len(chapters), 3), np.float64)\n",
    "    fvs_punct = np.zeros((len(chapters), 3), np.float64)\n",
    "    for e, ch_text in enumerate(chapters):\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        tokens = nltk.word_tokenize(ch_text.lower())\n",
    "        words = word_tokenizer.tokenize(ch_text.lower())\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                       for s in sentences])\n",
    "\n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # sentence length variation\n",
    "        fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    "\n",
    "        # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "        # Semicolons per sentence\n",
    "        fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "        # Colons per sentence\n",
    "        fvs_punct[e, 2] = tokens.count(':') / float(len(sentences))\n",
    "\n",
    "    # apply whitening to decorrelate the features\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train all new features using Multinomial BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>...</th>\n",
       "      <th>Number_of_Dots</th>\n",
       "      <th>Number_of_Commas</th>\n",
       "      <th>Number_of_Semicolons</th>\n",
       "      <th>Number_of_Colons</th>\n",
       "      <th>Avarage_Word_Lenght</th>\n",
       "      <th>POS_sentiment</th>\n",
       "      <th>NEG_sentiment</th>\n",
       "      <th>OverAll_sentiment</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Number_of_FunctionalWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           #AUTHID                       STATUS  \\\n",
       "0           1  b7b7764cfa1c523e4e93ab2a79a946c4  likes the sound of thunder.   \n",
       "\n",
       "   sEXT  sNEU  sAGR  sCON  sOPN cEXT cNEU            ...              \\\n",
       "0  2.65     3  3.15  3.25   4.4    n    y            ...               \n",
       "\n",
       "  Number_of_Dots Number_of_Commas Number_of_Semicolons Number_of_Colons  \\\n",
       "0              1                0                    0                0   \n",
       "\n",
       "   Avarage_Word_Lenght  POS_sentiment  NEG_sentiment  OverAll_sentiment  \\\n",
       "0                  4.4              0              0                  0   \n",
       "\n",
       "   Lexical_Diversity  Number_of_FunctionalWords  \n",
       "0                0.7                          2  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_n = pd.read_csv(\"../raw_data/data_n.csv\", parse_dates=True, infer_datetime_format=True)\n",
    "data_n.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>StringLength</th>\n",
       "      <th>Number_of_Words</th>\n",
       "      <th>Number_of_Dots</th>\n",
       "      <th>Number_of_Commas</th>\n",
       "      <th>Number_of_Semicolons</th>\n",
       "      <th>Number_of_Colons</th>\n",
       "      <th>Avarage_Word_Lenght</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        STATUS  cCON  cOPN  StringLength  Number_of_Words  \\\n",
       "0  likes the sound of thunder.     0     1            27                5   \n",
       "\n",
       "   Number_of_Dots  Number_of_Commas  Number_of_Semicolons  Number_of_Colons  \\\n",
       "0               1                 0                     0                 0   \n",
       "\n",
       "   Avarage_Word_Lenght  Lexical_Diversity  \n",
       "0                  4.4                0.7  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'n': 0, 'y': 1}\n",
    "# http://stackoverflow.com/a/17702781\n",
    "data_n = data_n.replace(d)\n",
    "new_fea = data_n[[\"STATUS\",\"cCON\",\"cOPN\",\"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Avarage_Word_Lenght\", \"Lexical_Diversity\"]]\n",
    "#a,b, = new_fea.data.shape\n",
    "new_fea.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new_fea, test_new_fea = sk.cross_validation.train_test_split(new_fea, train_size = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       383\n",
      "          1       0.13      1.00      0.23       435\n",
      "          2       0.00      0.00      0.00      1464\n",
      "          3       0.00      0.00      0.00      1090\n",
      "\n",
      "avg / total       0.02      0.13      0.03      3372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#clas_fit_NB = classifierNB.fit(train_new_fea[[\"Number_of_Commas\", \"StringLength\"]], train_new_fea[\"cOPN\"])\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(train_new_fea[[\"Number_of_Commas\", \"Number_of_Semicolons\",\"Number_of_Colons\",  \"StringLength\",\n",
    "                         \"Avarage_Word_Lenght\", \n",
    "                         \"Number_of_Words\",\"Number_of_Dots\", \n",
    "                         \"Lexical_Diversity\"]], train_new_fea[\"cOPN\"])\n",
    "predicted = model.predict(test_new_fea[[\"Number_of_Commas\", \"Number_of_Semicolons\",\"Number_of_Colons\", \n",
    "                                        \"Avarage_Word_Lenght\",\"StringLength\",\n",
    "                                        \"Number_of_Words\",\"Number_of_Dots\", \"Lexical_Diversity\"]])\n",
    "x = test_new_fea[[\"cOPN\",\"cCON\"]]\n",
    "\n",
    "def merge(a, b):\n",
    "    val = 0\n",
    "    if a == 0 and b == 0:\n",
    "        val = 0\n",
    "    if a == 0 and b == 1:\n",
    "        val = 1\n",
    "    if a == 1 and b == 0:\n",
    "        val = 2\n",
    "    if a == 1 and b == 1:\n",
    "        val = 3\n",
    "    return val\n",
    "    pass\n",
    "\n",
    "personality_trait_comb = [ merge(l[0], l[1]) for l in test_new_fea[[\"cOPN\", \"cCON\"]].values]\n",
    "\n",
    "print(metrics.classification_report(personality_trait_comb, predicted))\n",
    "# print(metrics.confusion_matrix(personality_trait_comb, predicted))\n",
    "\n",
    "#predicted_NB = classifierNB.predict(test_new_fea[[\"cCON\", \"cOPN\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVC: 0.619 (+/- 0.38)\n"
     ]
    }
   ],
   "source": [
    "scores_NB = cross_validation.cross_val_score(model, data_n[[\"Number_of_Commas\", \"Number_of_Semicolons\",\"Number_of_Colons\",  \"StringLength\",\"Avarage_Word_Lenght\", \"Number_of_Words\",\"Number_of_Dots\", \"Lexical_Diversity\"]], data_n[\"cOPN\"], cv = 10)\n",
    "print(\"Accuracy of Linear SVC: %0.3f (+/- %0.2f)\" % (scores_NB.mean(), scores_NB.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Union (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class for building sklearn Pipeline step. \n",
    "    This class should be used to select a column from a pandas data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_frame):\n",
    "        return data_frame[[self.column]]\n",
    "    \n",
    "pipelineUnion = Pipeline([\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            ('STATUS', Pipeline([\n",
    "                ('extract', ColumnSelector(column=\"STATUS\")),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range = (1,2)))\n",
    "            ])),\n",
    "\n",
    "            ('Number_of_Commas', Pipeline([\n",
    "                ('extract', ColumnSelector(column=\"Number_of_Commas\"))\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'STATUS': 0.6,\n",
    "            'Number_of_Commas': 0.5,\n",
    "        },\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('lsvc', LinearSVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f8192d90648d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# \"Number_of_Semicolons\",\"Number_of_Colons\", \"StringLength\", \"Avarage_Word_Lenght\", \"Number_of_Words\",\"Number_of_Dots\", \"Lexical_Diversity\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpipelineUnion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_new_fea\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_new_fea\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cOPN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#train_new_fea[[\"STATUS\", \"Number_of_Commas\"]], train_new_fea[\"cOPN\"].values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#y = pipelineUnion.predict(test_new_fea[[\"Number_of_Commas\", \"Number_of_Semicolons\",\"Number_of_Colons\",  \"StringLength\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \"\"\"\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_pre_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_transformer_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mXs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0mXs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \"\"\"\n\u001b[1;32m--> 464\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mbmat\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mbrow_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'blocks[%d,:] has incompatible row dimensions'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbcol_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: blocks[0,:] has incompatible row dimensions"
     ]
    }
   ],
   "source": [
    "# \"Number_of_Semicolons\",\"Number_of_Colons\", \"StringLength\", \"Avarage_Word_Lenght\", \"Number_of_Words\",\"Number_of_Dots\", \"Lexical_Diversity\"\n",
    "pipelineUnion.fit(train_new_fea, train_new_fea[\"cOPN\"])\n",
    "\n",
    "#train_new_fea[[\"STATUS\", \"Number_of_Commas\"]], train_new_fea[\"cOPN\"].values\n",
    "#y = pipelineUnion.predict(test_new_fea[[\"Number_of_Commas\", \"Number_of_Semicolons\",\"Number_of_Colons\",  \"StringLength\",\n",
    "#                          \"Avarage_Word_Lenght\", \n",
    "#                          \"Number_of_Words\",\"Number_of_Dots\", \n",
    "#                          \"Lexical_Diversity\"]])\n",
    "#print(classification_report(y, test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population: 112\n",
      "P: 45\n",
      "N: 67\n",
      "PositiveTest: 24\n",
      "NegativeTest: 88\n",
      "TP: 24\n",
      "TN: 67\n",
      "FP: 0\n",
      "FN: 21\n",
      "TPR: 0.533333333333\n",
      "TNR: 1.0\n",
      "PPV: 1.0\n",
      "NPV: 0.761363636364\n",
      "FPR: 0.0\n",
      "FDR: 0.0\n",
      "FNR: 0.466666666667\n",
      "ACC: 0.8125\n",
      "F1_score: 0.695652173913\n",
      "MCC: 0.637228849049\n",
      "informedness: 0.533333333333\n",
      "markedness: 0.761363636364\n",
      "prevalence: 0.401785714286\n",
      "LRP: inf\n",
      "LRN: 0.466666666667\n",
      "DOR: inf\n",
      "FOR: 0.238636363636\n",
      "0.860416666667\n",
      "0.8125\n",
      "0.1875\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      1.00      0.86        67\n",
      "          1       1.00      0.53      0.70        45\n",
      "\n",
      "avg / total       0.86      0.81      0.80       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_actu = [ 1,  1, 0, 0, 0,  1, 0,  1,  1,\n",
    "           0,  1, 0, 0, 0, 0, 0,  1, 0,\n",
    "            1,  1,  1,  1, 0, 0, 0,  1, 0,\n",
    "            1, 0, 0, 0, 0,  1,  1, 0, 0,\n",
    "           0,  1,  1,  1,  1, 0, 0, 0, 0,\n",
    "            1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0,  1,  1, 0,  1, 0,  1,  1,  1,\n",
    "           0, 0,  1, 0,  1, 0, 0,  1, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0,  1, 0,\n",
    "            1,  1,  1,  1, 0, 0,  1, 0,  1,\n",
    "            1, 0,  1, 0,  1, 0, 0,  1,  1,\n",
    "           0, 0,  1,  1, 0, 0, 0, 0, 0,\n",
    "           0,  1,  1, 0]\n",
    "\n",
    "y_pred = [0, 0, 0, 0, 0,  1, 0, 0,  1,\n",
    "       0,  1, 0, 0, 0, 0, 0, 0, 0,\n",
    "        1,  1,  1,  1, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0,  1, 0, 0, 0,\n",
    "       0,  1, 0, 0, 0, 0, 0, 0, 0,\n",
    "        1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0,  1, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0,  1, 0, 0, 0, 0,  1, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0,  1, 0,\n",
    "       0,  1, 0, 0, 0, 0,  1, 0,  1,\n",
    "        1, 0, 0, 0,  1, 0, 0,  1,  1,\n",
    "       0, 0,  1,  1, 0, 0, 0, 0, 0,\n",
    "       0,  1, 0, 0]\n",
    "\n",
    "binary_confusion_matrix = BinaryConfusionMatrix(y_actu, y_pred)\n",
    "# print(\"Binary confusion matrix:\\n%s\" % binary_confusion_matrix)\n",
    "binary_confusion_matrix.print_stats()\n",
    "print(sk.metrics.average_precision_score(y_actu, y_pred))\n",
    "print(sk.metrics.accuracy_score(y_actu, y_pred, normalize=True))\n",
    "print(sk.metrics.mean_absolute_error(y_actu, y_pred))\n",
    "# print(y_actu.shape, y_pred.shape)\n",
    "print(sk.metrics.classification_report(y_actu, y_pred))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
