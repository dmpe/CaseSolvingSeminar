{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n",
      "The cythonmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext cythonmagic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, reprlib, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import *\n",
    "import random as ran\n",
    "from logging import *\n",
    "from pprint import *\n",
    "from time import *\n",
    "import shlex, subprocess\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "ran.seed(5125)\n",
    "\n",
    "%load_ext cython\n",
    "%load_ext cythonmagic\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from pandas_confusion import *\n",
    "from scipy.cluster.vq import *\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem\n",
    "from nltk.corpus import *\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from nltk.probability import *\n",
    "from nltk.classify.scikitlearn import *\n",
    "from nltk.tag.sequential import *\n",
    "from nltk.tag import *\n",
    "from nltk.tag.util import *\n",
    "# n.download()\n",
    "\n",
    "from sklearn_pandas import *\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.datasets import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.grid_search import *\n",
    "from sklearn.base import *\n",
    "from sklearn.datasets.twenty_newsgroups import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_extraction import *\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R script adding new features to dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://www.mango-solutions.com/wp/2015/10/integrating-python-and-r-part-ii-executing-r-from-python-and-vice-versa/\n",
    "command = 'Rscript'\n",
    "path2script = 'insert_features.R'\n",
    "\n",
    "# Build subprocess command\n",
    "cmd = [command, path2script]\n",
    "\n",
    "subprocess.check_output(cmd, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>...</th>\n",
       "      <th>Number_of_Semicolons</th>\n",
       "      <th>Number_of_Colons</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>POS_sentiment</th>\n",
       "      <th>NEG_sentiment</th>\n",
       "      <th>OverAll_sentiment</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Number_of_FunctionalWords</th>\n",
       "      <th>Number_of_Pronouns</th>\n",
       "      <th>Number_of_PROPNAMEs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.577</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.509</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.611</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           #AUTHID  \\\n",
       "0           1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1           2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2           3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3           4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4           5  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65     3  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65     3  3.15  3.25   \n",
       "2  is sore and wants the knot of muscles at the b...  2.65     3  3.15  3.25   \n",
       "3         likes how the day sounds in this new song.  2.65     3  3.15  3.25   \n",
       "4                                        is home. <3  2.65     3  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU         ...          Number_of_Semicolons Number_of_Colons  \\\n",
       "0   4.4    n    y         ...                             0                0   \n",
       "1   4.4    n    y         ...                             0                0   \n",
       "2   4.4    n    y         ...                             0                0   \n",
       "3   4.4    n    y         ...                             0                0   \n",
       "4   4.4    n    y         ...                             0                0   \n",
       "\n",
       "  Average_Word_Length POS_sentiment  NEG_sentiment  OverAll_sentiment  \\\n",
       "0               4.400             0              0                  0   \n",
       "1               3.615             0              0                  0   \n",
       "2               3.500             0              2                 -2   \n",
       "3               3.667             0              0                  0   \n",
       "4               2.333             0              0                  0   \n",
       "\n",
       "   Lexical_Diversity  Number_of_FunctionalWords  Number_of_Pronouns  \\\n",
       "0              0.700                          2                   0   \n",
       "1              0.577                         10                   1   \n",
       "2              0.509                         14                   1   \n",
       "3              0.611                          4                   0   \n",
       "4              0.857                          1                   0   \n",
       "\n",
       "   Number_of_PROPNAMEs  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_n = pd.read_csv(\"../raw_data/data_n.csv\", parse_dates=True, infer_datetime_format=True)\n",
    "data_n.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace 'y' and 'n' with 1 & 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>...</th>\n",
       "      <th>Number_of_Semicolons</th>\n",
       "      <th>Number_of_Colons</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>POS_sentiment</th>\n",
       "      <th>NEG_sentiment</th>\n",
       "      <th>OverAll_sentiment</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Number_of_FunctionalWords</th>\n",
       "      <th>Number_of_Pronouns</th>\n",
       "      <th>Number_of_PROPNAMEs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           #AUTHID                       STATUS  \\\n",
       "0           1  b7b7764cfa1c523e4e93ab2a79a946c4  likes the sound of thunder.   \n",
       "\n",
       "   sEXT  sNEU  sAGR  sCON  sOPN  cEXT  cNEU         ...           \\\n",
       "0  2.65     3  3.15  3.25   4.4     0     1         ...            \n",
       "\n",
       "   Number_of_Semicolons  Number_of_Colons  Average_Word_Length POS_sentiment  \\\n",
       "0                     0                 0                  4.4             0   \n",
       "\n",
       "   NEG_sentiment  OverAll_sentiment  Lexical_Diversity  \\\n",
       "0              0                  0                0.7   \n",
       "\n",
       "   Number_of_FunctionalWords  Number_of_Pronouns  Number_of_PROPNAMEs  \n",
       "0                          2                   0                    0  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://stackoverflow.com/a/17702781\n",
    "d = {'n': 0, 'y': 1}\n",
    "data_n = data_n.replace(d)\n",
    "data_n.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#norm:http://blog.yhat.com/posts/predicting-customer-churn-with-sklearn.html\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "# to_drop = ['State','Area Code','Phone','Churn?']\n",
    "# churn_feat_space = data_n.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split stra. k-fold + selection of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neu = data_n[[\"#AUTHID\",\"STATUS\",\"cNEU\"]]\n",
    "# train_neu, test_neu, y_train, y_test = sk.cross_validation.train_test_split(neu, neu[\"cNEU\"], train_size = 0.66, stratify= neu[\"cNEU\"])\n",
    "# train_neu, test_neu, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neu = data_n[[\"#AUTHID\",\"STATUS\",\"cNEU\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]\n",
    "ext = data_n[[\"#AUTHID\",\"STATUS\",\"cEXT\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]\n",
    "agr = data_n[[\"#AUTHID\",\"STATUS\",\"cAGR\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]\n",
    "con = data_n[[\"#AUTHID\",\"STATUS\",\"cCON\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]\n",
    "opn = data_n[[\"#AUTHID\",\"STATUS\",\"cOPN\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feat_neu, test_feat_neu, train_class_neu, test_class_neu = sk.cross_validation.train_test_split(neu, neu[\"cNEU\"], train_size = 0.66, stratify = neu[\"cNEU\"])\n",
    "train_feat_ext, test_feat_ext, train_class_ext, test_class_ext = sk.cross_validation.train_test_split(ext, ext[\"cEXT\"], train_size = 0.66, stratify = ext[\"cEXT\"])\n",
    "train_feat_agr, test_feat_agr, train_class_agr, test_class_agr = sk.cross_validation.train_test_split(agr, agr[\"cAGR\"], train_size = 0.66, stratify = agr[\"cAGR\"])\n",
    "train_feat_con, test_feat_con, train_class_con, test_class_con = sk.cross_validation.train_test_split(con, con[\"cCON\"], train_size = 0.66, stratify = con[\"cCON\"])\n",
    "train_feat_opn, test_feat_opn, train_class_opn, test_class_opn = sk.cross_validation.train_test_split(opn, opn[\"cOPN\"], train_size = 0.66, stratify = opn[\"cOPN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mku = sk.pipeline.make_pipeline(SGDClassifier(), LinearSVC(),\n",
    "#                                # BernoulliNB(), #MultinomialNB()\n",
    "#                                # KNeighborsClassifier(),\n",
    "#               RandomForestClassifier(n_jobs=-1),AdaBoostClassifier()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train + Test SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ttSVC = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttSVC.fit(train_feat_neu[[\"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]], train_class_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testar = ttSVC.predict(test_feat_neu[[\"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted   0.0  1.0  __all__\n",
      "Actual                       \n",
      "0.0         605    0      605\n",
      "1.0         538    0      538\n",
      "__all__    1143    0     1143\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Accuracy: 0.529308836395\n",
      "95% CI: (0.4998908604288036, 0.55857561871616501)\n",
      "No Information Rate: ToDo\n",
      "P-Value [Acc > NIR]: 1.0\n",
      "Kappa: 0.0\n",
      "Mcnemar's Test P-Value: ToDo\n",
      "\n",
      "\n",
      "Class Statistics:\n",
      "\n",
      "Classes                                       0         1\n",
      "Population                                 1143      1143\n",
      "P: Condition positive                       605       538\n",
      "N: Condition negative                       538       605\n",
      "Test outcome positive                      1143         0\n",
      "Test outcome negative                         0      1143\n",
      "TP: True Positive                           605         0\n",
      "TN: True Negative                             0       605\n",
      "FP: False Positive                          538         0\n",
      "FN: False Negative                            0       538\n",
      "TPR: (Sensitivity, hit rate, recall)          1         0\n",
      "TNR=SPC: (Specificity)                        0         1\n",
      "PPV: Pos Pred Value (Precision)        0.529309       NaN\n",
      "NPV: Neg Pred Value                         NaN  0.529309\n",
      "FPR: False-out                                1         0\n",
      "FDR: False Discovery Rate              0.470691       NaN\n",
      "FNR: Miss Rate                                0         1\n",
      "ACC: Accuracy                          0.529309  0.529309\n",
      "F1 score                                0.69222         0\n",
      "MCC: Matthews correlation coefficient       NaN       NaN\n",
      "Informedness                                  0         0\n",
      "Markedness                                  NaN       NaN\n",
      "Prevalence                             0.529309  0.470691\n",
      "LR+: Positive likelihood ratio                1       NaN\n",
      "LR-: Negative likelihood ratio              NaN         1\n",
      "DOR: Diagnostic odds ratio                  NaN       NaN\n",
      "FOR: False omission rate                    NaN  0.470691\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(test_class_neu, testar)\n",
    "cm.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# new_fea = data_n[[\"STATUS\",\"cCON\",\"cOPN\",\"StringLength\", \n",
    "#                   \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "#                   \"Number_of_Colons\", \"Avarage_Word_Lenght\", \"Lexical_Diversity\"]]\n",
    "# new_fea.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predicted = mku.predict(test_neu[[\"StringLength\", \n",
    "#                   \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "#                   \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
