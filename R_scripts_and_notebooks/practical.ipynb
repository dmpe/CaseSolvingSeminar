{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n",
      "The cythonmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext cythonmagic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, reprlib, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import *\n",
    "import random as ran\n",
    "from logging import *\n",
    "from pprint import *\n",
    "from time import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ran.seed(5125)\n",
    "\n",
    "%load_ext cython\n",
    "%load_ext cythonmagic\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from pandas_confusion import *\n",
    "from scipy.cluster.vq import *\n",
    "\n",
    "import nltk as n\n",
    "import nltk, nltk.classify.util, nltk.metrics, nltk.tokenize, nltk.stem\n",
    "from nltk.corpus import *\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import BigramAssocMeasures as BAM\n",
    "from nltk.probability import *\n",
    "from nltk.classify.scikitlearn import *\n",
    "from nltk.tag.sequential import *\n",
    "from nltk.tag import *\n",
    "from nltk.tag.util import *\n",
    "\n",
    "\n",
    "from sklearn_pandas import *\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.datasets import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.grid_search import *\n",
    "from sklearn.base import *\n",
    "from sklearn.datasets.twenty_newsgroups import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_extraction import *\n",
    "from sklearn.metrics import *\n",
    "# n.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R script adding new features to dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f8dea7210b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath2script\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniversal_newlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# http://www.mango-solutions.com/wp/2015/10/integrating-python-and-r-part-ii-executing-r-from-python-and-vice-versa/\n",
    "command = 'Rscript'\n",
    "path2script = 'lexical_features.R'\n",
    "\n",
    "# Build subprocess command\n",
    "cmd = [command, path2script]\n",
    "\n",
    "subprocess.check_output(cmd, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>...</th>\n",
       "      <th>Number_of_Semicolons</th>\n",
       "      <th>Number_of_Colons</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>POS_sentiment</th>\n",
       "      <th>NEG_sentiment</th>\n",
       "      <th>OverAll_sentiment</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Number_of_FunctionalWords</th>\n",
       "      <th>Number_of_Pronouns</th>\n",
       "      <th>Number_of_PROPNAMEs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.577</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.509</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.611</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           #AUTHID  \\\n",
       "0           1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1           2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2           3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3           4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4           5  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65     3  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65     3  3.15  3.25   \n",
       "2  is sore and wants the knot of muscles at the b...  2.65     3  3.15  3.25   \n",
       "3         likes how the day sounds in this new song.  2.65     3  3.15  3.25   \n",
       "4                                        is home. <3  2.65     3  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU         ...          Number_of_Semicolons Number_of_Colons  \\\n",
       "0   4.4    n    y         ...                             0                0   \n",
       "1   4.4    n    y         ...                             0                0   \n",
       "2   4.4    n    y         ...                             0                0   \n",
       "3   4.4    n    y         ...                             0                0   \n",
       "4   4.4    n    y         ...                             0                0   \n",
       "\n",
       "  Average_Word_Length POS_sentiment  NEG_sentiment  OverAll_sentiment  \\\n",
       "0               4.400             0              0                  0   \n",
       "1               3.615             0              0                  0   \n",
       "2               3.500             0              2                 -2   \n",
       "3               3.667             0              0                  0   \n",
       "4               2.333             0              0                  0   \n",
       "\n",
       "   Lexical_Diversity  Number_of_FunctionalWords  Number_of_Pronouns  \\\n",
       "0              0.700                          2                   0   \n",
       "1              0.577                         10                   1   \n",
       "2              0.509                         14                   1   \n",
       "3              0.611                          4                   0   \n",
       "4              0.857                          1                   0   \n",
       "\n",
       "   Number_of_PROPNAMEs  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_n = pd.read_csv(\"../raw_data/data_n.csv\", parse_dates=True, infer_datetime_format=True)\n",
    "data_n.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace 'y' and 'n' with 1 & 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>...</th>\n",
       "      <th>Number_of_Semicolons</th>\n",
       "      <th>Number_of_Colons</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>POS_sentiment</th>\n",
       "      <th>NEG_sentiment</th>\n",
       "      <th>OverAll_sentiment</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Number_of_FunctionalWords</th>\n",
       "      <th>Number_of_Pronouns</th>\n",
       "      <th>Number_of_PROPNAMEs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           #AUTHID                       STATUS  \\\n",
       "0           1  b7b7764cfa1c523e4e93ab2a79a946c4  likes the sound of thunder.   \n",
       "\n",
       "   sEXT  sNEU  sAGR  sCON  sOPN  cEXT  cNEU         ...           \\\n",
       "0  2.65     3  3.15  3.25   4.4     0     1         ...            \n",
       "\n",
       "   Number_of_Semicolons  Number_of_Colons  Average_Word_Length POS_sentiment  \\\n",
       "0                     0                 0                  4.4             0   \n",
       "\n",
       "   NEG_sentiment  OverAll_sentiment  Lexical_Diversity  \\\n",
       "0              0                  0                0.7   \n",
       "\n",
       "   Number_of_FunctionalWords  Number_of_Pronouns  Number_of_PROPNAMEs  \n",
       "0                          2                   0                    0  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://stackoverflow.com/a/17702781\n",
    "d = {'n': 0, 'y': 1}\n",
    "data_n = data_n.replace(d)\n",
    "data_n.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#norm:http://blog.yhat.com/posts/predicting-customer-churn-with-sklearn.html\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "# to_drop = ['State','Area Code','Phone','Churn?']\n",
    "# churn_feat_space = data_n.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split stra. k-fold + selection of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neu = data_n[[\"#AUTHID\",\"STATUS\",\"cNEU\"]]\n",
    "# train_neu, test_neu, y_train, y_test = sk.cross_validation.train_test_split(neu, neu[\"cNEU\"], train_size = 0.66, stratify= neu[\"cNEU\"])\n",
    "# train_neu, test_neu, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neu = data_n[[\"#AUTHID\",\"STATUS\",\"cNEU\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]\n",
    "ext = data_n[[\"#AUTHID\",\"STATUS\",\"cEXT\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]\n",
    "agr = data_n[[\"#AUTHID\",\"STATUS\",\"cAGR\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]\n",
    "con = data_n[[\"#AUTHID\",\"STATUS\",\"cCON\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]\n",
    "opn = data_n[[\"#AUTHID\",\"STATUS\",\"cOPN\", \"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feat_neu, test_feat_neu, train_class_neu, test_class_neu = sk.cross_validation.train_test_split(neu, neu[\"cNEU\"], train_size = 0.66, stratify = neu[\"cNEU\"])\n",
    "train_ext, test_ext = sk.cross_validation.train_test_split(ext, train_size = 0.66, stratify= ext[\"cEXT\"])\n",
    "train_agr, test_agr = sk.cross_validation.train_test_split(agr, train_size = 0.66, stratify= agr[\"cAGR\"])\n",
    "train_con, test_con = sk.cross_validation.train_test_split(con, train_size = 0.66, stratify= con[\"cCON\"])\n",
    "train_opn, test_opn = sk.cross_validation.train_test_split(opn, train_size = 0.66, stratify= opn[\"cOPN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mku = sk.pipeline.make_pipeline(SGDClassifier(), LinearSVC(),\n",
    "                               # BernoulliNB(), #MultinomialNB()\n",
    "                               # KNeighborsClassifier(),\n",
    "              RandomForestClassifier(n_jobs=-1),AdaBoostClassifier()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('sgdclassifier', SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=...m='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mku.fit(train_feat_neu[[\"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]], train_class_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testar = mku.predict(test_feat_neu[[\"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predicted = mku.predict(test_neu[[\"StringLength\", \n",
    "#                   \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "#                   \"Number_of_Colons\", \"Average_Word_Length\", \"Lexical_Diversity\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Avarage_Word_Lenght'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c46c0328131f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m new_fea = data_n[[\"STATUS\",\"cCON\",\"cOPN\",\"StringLength\", \n\u001b[0;32m      2\u001b[0m                   \u001b[1;34m\"Number_of_Words\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Number_of_Dots\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Number_of_Commas\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Number_of_Semicolons\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                   \"Number_of_Colons\", \"Avarage_Word_Lenght\", \"Lexical_Diversity\"]]\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnew_fea\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1964\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1965\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2005\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2007\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2008\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s not in index'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Avarage_Word_Lenght'] not in index\""
     ]
    }
   ],
   "source": [
    "new_fea = data_n[[\"STATUS\",\"cCON\",\"cOPN\",\"StringLength\", \n",
    "                  \"Number_of_Words\", \"Number_of_Dots\", \"Number_of_Commas\", \"Number_of_Semicolons\", \n",
    "                  \"Number_of_Colons\", \"Avarage_Word_Lenght\", \"Lexical_Diversity\"]]\n",
    "new_fea.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted   0.0  1.0  __all__\n",
      "Actual                       \n",
      "0.0         621    1      622\n",
      "1.0         534    2      536\n",
      "__all__    1155    3     1158\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "\n",
      "Accuracy: 0.537996545769\n",
      "95% CI: (0.50878419260463481, 0.56701544430656814)\n",
      "No Information Rate: ToDo\n",
      "P-Value [Acc > NIR]: 1.0\n",
      "Kappa: 0.00228039153163\n",
      "Mcnemar's Test P-Value: ToDo\n",
      "\n",
      "\n",
      "Class Statistics:\n",
      "\n",
      "Classes                                         0           1\n",
      "Population                                   1158        1158\n",
      "P: Condition positive                         622         536\n",
      "N: Condition negative                         536         622\n",
      "Test outcome positive                        1155           3\n",
      "Test outcome negative                           3        1155\n",
      "TP: True Positive                             621           2\n",
      "TN: True Negative                               2         621\n",
      "FP: False Positive                            534           1\n",
      "FN: False Negative                              1         534\n",
      "TPR: (Sensitivity, hit rate, recall)     0.998392  0.00373134\n",
      "TNR=SPC: (Specificity)                 0.00373134    0.998392\n",
      "PPV: Pos Pred Value (Precision)          0.537662    0.666667\n",
      "NPV: Neg Pred Value                      0.666667    0.537662\n",
      "FPR: False-out                           0.996269  0.00160772\n",
      "FDR: False Discovery Rate                0.462338    0.333333\n",
      "FNR: Miss Rate                         0.00160772    0.996269\n",
      "ACC: Accuracy                            0.537997    0.537997\n",
      "F1 score                                 0.698931  0.00742115\n",
      "MCC: Matthews correlation coefficient   0.0208307   0.0208307\n",
      "Informedness                           0.00212363  0.00212363\n",
      "Markedness                               0.204329    0.204329\n",
      "Prevalence                               0.537133    0.462867\n",
      "LR+: Positive likelihood ratio            1.00213      2.3209\n",
      "LR-: Negative likelihood ratio           0.430868    0.997873\n",
      "DOR: Diagnostic odds ratio                2.32584     2.32584\n",
      "FOR: False omission rate                 0.333333    0.462338\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(test_class_neu, testar)\n",
    "cm.print_stats()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
