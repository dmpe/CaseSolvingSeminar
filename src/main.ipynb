{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import sklearn as sk\n",
    "\n",
    "import cssfeature\n",
    "import csspipe\n",
    "import csstransformer\n",
    "\n",
    "import helper\n",
    "\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load data\n",
    "(run bash ./src/init.sh first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>cNeu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  cNeu\n",
       "0                        likes the sound of thunder.  True\n",
       "1  is so sleepy it's not even funny that's she ca...  True\n",
       "2  is sore and wants the knot of muscles at the b...  True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_csv(\"../raw_data/data_n.csv\", parse_dates=True, infer_datetime_format=True)\n",
    "#help(data.head)\n",
    "#data.head(1)\n",
    "db_path = './data.sqlite3'\n",
    "sqlite_connection = sqlite3.connect(db_path)\n",
    "data_reader = helper.CNeuReader(sqlite_connection)\n",
    "\n",
    "data = data_reader.get_results()\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# Create DataFrame with string-length-featues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  cNeu\n",
      "0                        likes the sound of thunder.  True\n",
      "1  is so sleepy it's not even funny that's she ca...  True\n",
      "                                            sentence  cNeu\n",
      "0                        likes the sound of thunder.  True\n",
      "1  is so sleepy it's not even funny that's she ca...  True\n",
      "\n",
      "  sentence_length  cNeu\n",
      "0              27  True\n",
      "1              63  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data.head(2))\n",
    "\n",
    "## create length of sentence feature\n",
    "sl_data = cssfeature.CharacterFeatures.string_length(data, column=0, column_name='sentence_length')\n",
    "\n",
    "print(data.head(2))\n",
    "print()\n",
    "print(sl_data.head(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentence_length\n",
      "0              27\n",
      "1              63\n",
      "   cNeu\n",
      "0  True\n",
      "1  True\n"
     ]
    }
   ],
   "source": [
    "features = sl_data[['sentence_length']].astype(str)\n",
    "labels = sl_data[['cNeu']].astype(bool)\n",
    "\n",
    "print(features.head(2))\n",
    "print(labels.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"train_feat\\n%s\\n\" %train_feat.head(1))\\nprint(\"train_label\\n%s\\n\" %train_label.head(1))\\nprint(\"test_feat\\n%s\\n\" %test_feat.head(1))\\nprint(\"test_label\\n%s\\n\" %test_label.head(1))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split = sk.cross_validation.train_test_split(features, labels, train_size = 0.66, random_state= 6432119)\n",
    "\n",
    "train_feat, test_feat, train_label, test_label = split\n",
    "\n",
    "\"\"\"\n",
    "print(\"train_feat\\n%s\\n\" %train_feat.head(1))\n",
    "print(\"train_label\\n%s\\n\" %train_label.head(1))\n",
    "print(\"test_feat\\n%s\\n\" %test_feat.head(1))\n",
    "print(\"test_label\\n%s\\n\" %test_label.head(1))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pipefactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipefactory = csspipe.PipeFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create classifiers for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "multinomial_nb = pipefactory.classifiers.multinomial_nb\n",
    "bernoulli_nb = pipefactory.classifiers.bernoulli_nb\n",
    "\n",
    "svc = pipefactory.classifiers.svc\n",
    "linear_svc = pipefactory.classifiers.linear_svc\n",
    "\n",
    "random_forest = pipefactory.classifiers.random_forest_classifier\n",
    "k_neighbors = pipefactory.classifiers.k_neighors_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create extractors for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = pipefactory.extractors.tfidf_vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create selectors for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_best = pipefactory.selectors.select_k_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tag\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "postagger = nltk.tag.ClassifierBasedPOSTagger(train=treebank.tagged_sents())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mytagger = csstransformer.PartOfSpeech(ignore_unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[('l', 'RB'), ('i', 'PRP'), ('k', 'VBP'), ('e', 'RB'), ('s', 'RBR'), (' ', '#'), ('t', 'JJS'), ('h', 'PDT'), ('e', 'WP$'), (' ', '#'), ('s', 'RBR'), ('o', 'WP'), ('u', 'PDT'), ('n', 'WRB'), ('d', 'PDT'), (' ', '#'), ('o', 'WP'), ('f', 'PDT'), (' ', '#'), ('t', 'RBS'), ('h', 'PDT'), ('u', 'FW'), ('n', 'WRB'), ('d', 'PDT'), ('e', 'WP$'), ('r', 'RBR'), ('.', '.')], [('i', 'PRP'), ('s', 'VBD'), (' ', '#'), ('s', 'PRP$'), ('o', 'WP'), (' ', '#'), ('s', 'VBZ'), ('l', 'PDT'), ('e', 'EX'), ('e', 'WP$'), ('p', 'RP'), ('y', 'WRB'), (' ', '#'), ('i', 'PRP'), ('t', 'VBP'), (\"'\", \"''\"), ('s', 'PRP$'), (' ', '#'), ('n', 'WRB'), ('o', 'WP'), ('t', 'RBS'), (' ', '#'), ('e', 'WP$'), ('v', 'WP$'), ('e', 'WP$'), ('n', 'WRB'), (' ', '#'), ('f', 'PDT'), ('u', 'FW'), ('n', 'WRB'), ('n', 'WRB'), ('y', 'PRP'), (' ', '#'), ('t', 'JJS'), ('h', 'PDT'), ('a', 'DT'), ('t', 'JJS'), (\"'\", 'POS'), ('s', 'NNS'), (' ', ':'), ('s', 'PRP$'), ('h', 'PDT'), ('e', 'RBR'), (' ', '#'), ('c', 'WP$'), ('a', 'DT'), ('n', 'JJ'), (\"'\", 'POS'), ('t', 'JJS'), (' ', '#'), ('g', 'WP$'), ('e', 'WP$'), ('t', 'RBS'), (' ', '#'), ('t', 'RBS'), ('o', 'WP'), (' ', '#'), ('s', 'VBZ'), ('l', 'PDT'), ('e', 'EX'), ('e', 'WP$'), ('p', 'RP'), ('.', '.')], [('i', 'PRP'), ('s', 'VBD'), (' ', '#'), ('s', 'PRP$'), ('o', 'WP'), ('r', 'JJR'), ('e', 'WP$'), (' ', '#'), ('a', 'DT'), ('n', 'CD'), ('d', 'CC'), (' ', '#'), ('w', 'WRB'), ('a', 'DT'), ('n', 'JJ'), ('t', 'CC'), ('s', 'PRP$'), (' ', '#'), ('t', 'RBS'), ('h', 'PDT'), ('e', 'WP$'), (' ', '#'), ('k', 'WP$'), ('n', 'WRB'), ('o', 'WP'), ('t', 'RBS'), (' ', '#'), ('o', 'WP'), ('f', 'PDT'), (' ', '#'), ('m', 'WP$'), ('u', 'FW'), ('s', 'RBR'), ('c', 'WP$'), ('l', 'PDT'), ('e', 'WP$'), ('s', 'RBR'), (' ', '#'), ('a', 'DT'), ('t', 'JJS'), (' ', ':'), ('t', 'WP'), ('h', 'PDT'), ('e', 'WP$'), (' ', '#'), ('b', 'LS'), ('a', 'DT'), ('s', 'JJR'), ('e', 'RBR'), (' ', '#'), ('o', 'WP'), ('f', 'PDT'), (' ', '#'), ('h', 'PDT'), ('e', 'WP$'), ('r', 'RBR'), (' ', '#'), ('n', 'WRB'), ('e', 'EX'), ('c', 'WP$'), ('k', 'WP$'), (' ', '#'), ('t', 'RBS'), ('o', 'WP'), (' ', '#'), ('s', 'VBZ'), ('t', 'RP'), ('o', 'TO'), ('p', 'RP'), (' ', '#'), ('h', 'PDT'), ('u', 'FW'), ('r', 'RBR'), ('t', 'RBS'), ('i', 'PRP'), ('n', 'MD'), ('g', 'VB'), ('.', '.'), (' ', \"''\"), ('O', 'UH'), ('n', 'WRB'), (' ', '#'), ('t', 'JJS'), ('h', 'PDT'), ('e', 'WP$'), (' ', '#'), ('o', 'WP'), ('t', 'RBS'), ('h', 'PDT'), ('e', 'WP$'), ('r', 'RBR'), (' ', '#'), ('h', 'PDT'), ('a', 'DT'), ('n', 'JJ'), ('d', 'CC'), (',', ','), (' ', \"''\"), ('Y', 'PRP'), ('A', 'DT'), ('Y', 'JJ'), (' ', ':'), ('I', 'PRP'), (\"'\", 'POS'), ('M', 'UH'), (' ', '#'), ('I', 'PRP'), ('N', 'MD'), (' ', '#'), ('I', 'PRP'), ('L', 'MD'), ('L', 'PDT'), ('I', 'PRP'), ('N', 'MD'), ('O', 'RB'), ('I', 'PRP'), ('S', 'VBZ'), ('!', '.'), (' ', \"''\"), ('<', '#'), ('3', 'CD')], [('l', 'RB'), ('i', 'PRP'), ('k', 'VBP'), ('e', 'RB'), ('s', 'RBR'), (' ', '#'), ('h', 'PDT'), ('o', 'WP'), ('w', 'WRB'), (' ', '#'), ('t', 'JJS'), ('h', 'PDT'), ('e', 'WP$'), (' ', '#'), ('d', 'WP$'), ('a', 'DT'), ('y', 'JJ'), (' ', ':'), ('s', 'VBZ'), ('o', 'WP'), ('u', 'PRP'), ('n', 'MD'), ('d', 'VB'), ('s', 'PRP$'), (' ', '#'), ('i', 'PRP'), ('n', 'MD'), (' ', '#'), ('t', 'RBS'), ('h', 'PDT'), ('i', 'PRP'), ('s', 'VBZ'), (' ', '#'), ('n', 'RP'), ('e', 'WP$'), ('w', 'WRB'), (' ', '#'), ('s', 'RBR'), ('o', 'WP'), ('n', 'WRB'), ('g', 'PDT'), ('.', '.')], [('i', 'PRP'), ('s', 'VBD'), (' ', '#'), ('h', 'PDT'), ('o', 'WP'), ('m', 'PDT'), ('e', 'WP$'), ('.', '.'), (' ', \"''\"), ('<', '#'), ('3', 'CD')]]\n",
      "\n",
      "0                                        DT NN IN NN .\n",
      "1    VBZ RB  PRP POS  RB RB JJ IN POS  PRP MD POS  ...\n",
      "2    VBZ  CC VBZ DT  IN  IN DT NN IN PRP$  TO VB VB...\n",
      "3                            WRB DT NN NNS IN DT JJ  .\n",
      "4                                         VBZ NN .  CD\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#postagger.evaluate(treebank.tagged_sents())\n",
    "input = data.sentence[:5,]\n",
    "print()\n",
    "print(postagger.tag_sents(input))\n",
    "print()\n",
    "print(mytagger.transform(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Pipelineing\n",
    "# http://scikit-learn.org/stable/modules/pipeline.html\n",
    "# http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "\n",
    "class Test(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, input):\n",
    "        return postagger.tag_sents(input)\n",
    "\n",
    "\n",
    "\n",
    "raw_pipeline = pipefactory.create_pipe([\n",
    "        #tfidf_vectorizer\n",
    "        ('tfidf', sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,2), analyzer='char'))\n",
    "        #, ('kNN', sklearn.neighbors.KNeighborsClassifier())\n",
    "        , multinomial_nb\n",
    "    ])\n",
    "\n",
    "pos_pipeline = pipefactory.create_pipe([\n",
    "        #tfidf_vectorizer\n",
    "        ('part of speech', Test())\n",
    "        #('part of speech', csstransformer.PartOfSpeech(ignore_unknown=True))\n",
    "        , ('tfidf', sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,2), analyzer='char'))\n",
    "        #, ('kNN', sklearn.neighbors.KNeighborsClassifier())\n",
    "        , multinomial_nb\n",
    "    ])\n",
    "\n",
    "data = helper.CNeuReader(sqlite_connection).get_results()\n",
    "#print(data.head())\n",
    "\n",
    "#print(data['sentence'].astype(str).values)\n",
    "#print(data['cNeu'].astype(str).values)\n",
    "\n",
    "#help(pipeline.fit(data['sentence'].astype(str).values, data['cNeu'].astype(bool).values))\n",
    "#pipeline.fit(data['sentence'].astype(str).values, data['cNeu'].astype(bool).values)\n",
    "\n",
    "\n",
    "r1 = sk.cross_validation.cross_val_score(raw_pipeline, data['sentence'].astype(str), data['cNeu'].astype(bool), cv=10)\n",
    "r2 = sk.cross_validation.cross_val_score(pos_pipeline, data['sentence'].astype(str), data['cNeu'].astype(bool), cv=10)\n",
    "\n",
    "print(r1.mean())\n",
    "print(r2.mean())\n",
    "\n",
    "print(pos_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data:\n",
      "['likes the sound of thunder.'\n",
      " \"is so sleepy it's not even funny that's she can't get to sleep.\"\n",
      " \"is sore and wants the knot of muscles at the base of her neck to stop hurting. On the other hand, YAY I'M IN ILLINOIS! <3\"]\n",
      "['and', 'and wants', 'at', 'at the', 'base', 'base of', 'can', 'can get', 'even', 'even funny']\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "test_data = data['sentence'].astype(str).values[:3]\n",
    "\n",
    "print(\"test_data:\\n%s\" % test_data)\n",
    "\n",
    "tfidf_trans = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,2))\n",
    "#help(tfidf_trans)\n",
    "tfidf_trans.fit(test_data)\n",
    "#tfidf_trans.transform(test_data)\n",
    "\n",
    "print(tfidf_trans.get_feature_names()[:10])\n",
    "\n",
    "type(tfidf_trans.transform(data))\n",
    "print(tfidf_trans.transform([\"funny\", \"and\"]).toarray())\n",
    "\n",
    "#tfidf_trans.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "select_k has transform: True\n",
      "select_k has fit: True\n",
      "select_k has fit_transform: True\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = sk.feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "pipeline = sk.pipeline.Pipeline([('tfidf', tfidf_vectorizer),\n",
    "                     ('chi2', sk.feature_selection.SelectKBest(sk.feature_selection.chi2, k=1000)),\n",
    "                     ('nb', sk.naive_bayes.MultinomialNB())\n",
    "                    ])\n",
    "\n",
    "#pipeline.fit(train_feat[[\"STATUS\"]], train_label[[\"cNEU\"]])\n",
    "#data = train_feat.STATUS\n",
    "#target = train_label.cNEU.apply(lambda x: x == 'y')\n",
    "\n",
    "#foo = tfidf_vectorizer.fit_transform(data[:1], target[:1])\n",
    "#print(foo)\n",
    "#idf = foo.idf_\n",
    "\n",
    "#print(len(idf))\n",
    "#print(len(data))\n",
    "\n",
    "#pipeline.fit(data.sentence, data.cNeu)\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "\n",
    "data = helper.CNeuReader(sqlite_connection).get_results()\n",
    "\n",
    "features = data.sentence\n",
    "labels = data.cNeu\n",
    "\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer()\n",
    "\n",
    "cv_fit = cv.fit(features, labels)\n",
    "cv_transform_features = cv_fit.transform(features)\n",
    "\n",
    "\n",
    "tfidftrans = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "\n",
    "tfidftrans_fit = tfidftrans.fit(cv_transform_features, labels)\n",
    "tfidftrans_transform_features = tfidftrans_fit.transform(cv_transform_features)\n",
    "\n",
    "#print(cv_transform_features)\n",
    "#print(tfidftrans_transform_features)\n",
    "\n",
    "skb = sk.feature_selection.SelectKBest(sk.feature_selection.chi2, k=1000)\n",
    "mnb = sk.naive_bayes.MultinomialNB()\n",
    "\n",
    "\n",
    "\n",
    "mnb_fit = mnb.fit(tfidftrans_transform_features, labels.astype(bool))\n",
    "\n",
    "print(hasattr(mnb_fit, \"fit\"))\n",
    "print(hasattr(mnb_fit, \"transform\"))\n",
    "print(hasattr(mnb_fit, \"fit_transform\"))\n",
    "\n",
    "pipe = sklearn.pipeline.Pipeline([\n",
    "    ('cv', sklearn.feature_extraction.text.CountVectorizer())\n",
    "    , ('tfidf_trans', sklearn.feature_extraction.text.TfidfTransformer())\n",
    "    , ('nb', sk.naive_bayes.MultinomialNB())\n",
    "])\n",
    "\n",
    "sk.cross_validation.cross_val_score(pipe, features.astype(str), labels.astype(bool))\n",
    "\n",
    "\n",
    "\n",
    "#print(tfidf_vectorizer.fit_transform(features, labels))\n",
    "#tfidf = tfidf_vectorizer.fit_transform(features, labels)\n",
    "#print(tfidf)\n",
    "\n",
    "#print(tfidf[[9]])\n",
    "\n",
    "\n",
    "select_k = sk.feature_selection.SelectKBest(sk.feature_selection.chi2, k=1000)\n",
    "print('select_k has transform: %s' % hasattr(select_k, 'transform'))\n",
    "print('select_k has fit: %s' % hasattr(select_k, 'fit'))\n",
    "print('select_k has fit_transform: %s' % hasattr(select_k, 'fit_transform'))\n",
    "\n",
    "\n",
    "#help(tfidf_vectorizer)\n",
    "\n",
    "#result = sk.cross_validation.cross_val_score(pipeline, data[[0]], data[[1]])\n",
    "\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_factory = csspipe.PipeFactory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_nb_pipe = pipe_factory.create_tfidf_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_nb_pipe.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-50f6235b574f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_nb_pipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m   1431\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m                                               fit_params)\n\u001b[1;32m-> 1433\u001b[1;33m                       for train, test in cv)\n\u001b[0m\u001b[0;32m   1434\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    660\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \"\"\"\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_pre_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1303\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \"\"\"\n\u001b[1;32m-> 1305\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1306\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 817\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 238\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "result = sk.cross_validation.cross_val_score(tfidf_nb_pipe, data[[0]].values, data[[1]].values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
