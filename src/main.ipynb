{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "import csspipe\n",
    "import cssfeature\n",
    "\n",
    "import helper\n",
    "\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load data\n",
    "(run bash ./src/init.sh first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>cNeu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  cNeu\n",
       "0                        likes the sound of thunder.  True\n",
       "1  is so sleepy it's not even funny that's she ca...  True\n",
       "2  is sore and wants the knot of muscles at the b...  True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_csv(\"../raw_data/data_n.csv\", parse_dates=True, infer_datetime_format=True)\n",
    "#help(data.head)\n",
    "#data.head(1)\n",
    "db_path = './data.sqlite3'\n",
    "sqlite_connection = sqlite3.connect(db_path)\n",
    "data_reader = helper.CNeuReader(sqlite_connection)\n",
    "\n",
    "data = data_reader.get_results()\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# Create DataFrame with string-length-featues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  cNeu\n",
      "0                        likes the sound of thunder.  True\n",
      "1  is so sleepy it's not even funny that's she ca...  True\n",
      "                                            sentence  cNeu\n",
      "0                        likes the sound of thunder.  True\n",
      "1  is so sleepy it's not even funny that's she ca...  True\n",
      "\n",
      "  sentence_length  cNeu\n",
      "0              27  True\n",
      "1              63  True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data.head(2))\n",
    "\n",
    "## create length of sentence feature\n",
    "sl_data = cssfeature.CharacterFeatures.string_length(data, column=0, column_name='sentence_length')\n",
    "\n",
    "print(data.head(2))\n",
    "print()\n",
    "print(sl_data.head(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentence_length\n",
      "0              27\n",
      "1              63\n",
      "   cNeu\n",
      "0  True\n",
      "1  True\n"
     ]
    }
   ],
   "source": [
    "features = sl_data[['sentence_length']].astype(str)\n",
    "labels = sl_data[['cNeu']].astype(bool)\n",
    "\n",
    "print(featues.head(2))\n",
    "print(labels.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"train_feat\\n%s\\n\" %train_feat.head(1))\\nprint(\"train_label\\n%s\\n\" %train_label.head(1))\\nprint(\"test_feat\\n%s\\n\" %test_feat.head(1))\\nprint(\"test_label\\n%s\\n\" %test_label.head(1))\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split = sk.cross_validation.train_test_split(features, labels, train_size = 0.66, random_state= 6432119)\n",
    "\n",
    "train_feat, test_feat, train_label, test_label = split\n",
    "\n",
    "\"\"\"\n",
    "print(\"train_feat\\n%s\\n\" %train_feat.head(1))\n",
    "print(\"train_label\\n%s\\n\" %train_label.head(1))\n",
    "print(\"test_feat\\n%s\\n\" %test_feat.head(1))\n",
    "print(\"test_label\\n%s\\n\" %test_label.head(1))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pipefactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipefactory = csspipe.PipeFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create classifiers for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "multinomial_nb = pipefactory.classifiers.multinomial_nb\n",
    "bernoulli_nb = pipefactory.classifiers.bernoulli_nb\n",
    "\n",
    "svc = pipefactory.classifiers.svc\n",
    "linear_svc = pipefactory.classifiers.linear_svc\n",
    "\n",
    "random_forest = pipefactory.classifiers.random_forest_classifier\n",
    "k_neighbors = pipefactory.classifiers.k_neighors_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create extractors for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = pipefactory.extractors.tfidf_vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create selectors for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_best = pipefactory.selectors.select_k_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.63006655,  0.61161525,  0.62027231])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pipelineing\n",
    "# http://scikit-learn.org/stable/modules/pipeline.html\n",
    "# http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "\n",
    "pipeline = pipefactory.create_pipe([\n",
    "        tfidf_vectorizer\n",
    "        , multinomial_nb\n",
    "    ])\n",
    "\n",
    "data = helper.CNeuReader(sqlite_connection).get_results()\n",
    "#print(data.head())\n",
    "\n",
    "#print(data['sentence'].astype(str).values)\n",
    "#print(data['cNeu'].astype(str).values)\n",
    "\n",
    "sk.cross_validation.cross_val_score(pipeline, data['sentence'].astype(str).values, data['cNeu'].astype(bool).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "select_k has transform: True\n",
      "select_k has fit: True\n",
      "select_k has fit_transform: True\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = sk.feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "pipeline = sk.pipeline.Pipeline([('tfidf', tfidf_vectorizer),\n",
    "                     ('chi2', sk.feature_selection.SelectKBest(sk.feature_selection.chi2, k=1000)),\n",
    "                     ('nb', sk.naive_bayes.MultinomialNB())\n",
    "                    ])\n",
    "\n",
    "#pipeline.fit(train_feat[[\"STATUS\"]], train_label[[\"cNEU\"]])\n",
    "#data = train_feat.STATUS\n",
    "#target = train_label.cNEU.apply(lambda x: x == 'y')\n",
    "\n",
    "#foo = tfidf_vectorizer.fit_transform(data[:1], target[:1])\n",
    "#print(foo)\n",
    "#idf = foo.idf_\n",
    "\n",
    "#print(len(idf))\n",
    "#print(len(data))\n",
    "\n",
    "#pipeline.fit(data.sentence, data.cNeu)\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "\n",
    "data = helper.CNeuReader(sqlite_connection).get_results()\n",
    "\n",
    "features = data.sentence\n",
    "labels = data.cNeu\n",
    "\n",
    "cv = sklearn.feature_extraction.text.CountVectorizer()\n",
    "\n",
    "cv_fit = cv.fit(features, labels)\n",
    "cv_transform_features = cv_fit.transform(features)\n",
    "\n",
    "\n",
    "tfidftrans = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "\n",
    "tfidftrans_fit = tfidftrans.fit(cv_transform_features, labels)\n",
    "tfidftrans_transform_features = tfidftrans_fit.transform(cv_transform_features)\n",
    "\n",
    "#print(cv_transform_features)\n",
    "#print(tfidftrans_transform_features)\n",
    "\n",
    "skb = sk.feature_selection.SelectKBest(sk.feature_selection.chi2, k=1000)\n",
    "mnb = sk.naive_bayes.MultinomialNB()\n",
    "\n",
    "\n",
    "\n",
    "mnb_fit = mnb.fit(tfidftrans_transform_features, labels.astype(bool))\n",
    "\n",
    "print(hasattr(mnb_fit, \"fit\"))\n",
    "print(hasattr(mnb_fit, \"transform\"))\n",
    "print(hasattr(mnb_fit, \"fit_transform\"))\n",
    "\n",
    "pipe = sklearn.pipeline.Pipeline([\n",
    "    ('cv', sklearn.feature_extraction.text.CountVectorizer())\n",
    "    , ('tfidf_trans', sklearn.feature_extraction.text.TfidfTransformer())\n",
    "    , ('nb', sk.naive_bayes.MultinomialNB())\n",
    "])\n",
    "\n",
    "sk.cross_validation.cross_val_score(pipe, features.astype(str), labels.astype(bool))\n",
    "\n",
    "\n",
    "\n",
    "#print(tfidf_vectorizer.fit_transform(features, labels))\n",
    "#tfidf = tfidf_vectorizer.fit_transform(features, labels)\n",
    "#print(tfidf)\n",
    "\n",
    "#print(tfidf[[9]])\n",
    "\n",
    "\n",
    "select_k = sk.feature_selection.SelectKBest(sk.feature_selection.chi2, k=1000)\n",
    "print('select_k has transform: %s' % hasattr(select_k, 'transform'))\n",
    "print('select_k has fit: %s' % hasattr(select_k, 'fit'))\n",
    "print('select_k has fit_transform: %s' % hasattr(select_k, 'fit_transform'))\n",
    "\n",
    "\n",
    "#help(tfidf_vectorizer)\n",
    "\n",
    "#result = sk.cross_validation.cross_val_score(pipeline, data[[0]], data[[1]])\n",
    "\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_factory = csspipe.PipeFactory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_nb_pipe = pipe_factory.create_tfidf_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_nb_pipe.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-f135b620fea8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_nb_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1425\u001b[1;33m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1426\u001b[0m     \u001b[1;31m# We clone the estimator to make sure that all the folds are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[1;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         raise TypeError(\"estimator should a be an estimator implementing \"\n\u001b[1;32m--> 236\u001b[1;33m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[0m\u001b[0;32m    237\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhas_scoring\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "result = sk.cross_validation.cross_val_score(tfidf_nb_pipe.steps[0], data[[0]], data[[1]])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
